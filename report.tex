\documentclass[a4paper,11pt]{report}

%\usepackage[margin=1in]{geometry}

% This sets the margins. They can be reduced to 20mm if i need the room later!
\usepackage{geometry}
 \geometry{
 a4paper,
 left=20mm,
 top=30mm,
 }

\author{Constance Crozier\\Department of Engineering Science, University of Oxford}
\title{Bayesian Non-Parametrics for Data from the War in Afghanistan}
\date{May 2016}

% In order to import matlab figures
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

% This sets the font to arial
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

% This changes the heading distances at the start of chapters
\usepackage{titlesec}
\titleformat{\chapter}[display]   
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{20pt}{\Huge}   
\titlespacing*{\chapter}{0pt}{-60pt}{15pt}

% This changes the form of the chapter title
\usepackage[T1]{fontenc}
\usepackage{titlesec, blindtext}
\newcommand{\hsp}{\hspace{20pt}}
\titleformat{\chapter}[hang]{\LARGE\bfseries}{\thechapter\hsp}{0pt}{\LARGE\bfseries}
\titleformat{\section}[hang]{\large\bfseries}{\thesection\hsp}{0pt}{\large\bfseries}

% This lets us use double spacing
\usepackage{setspace}
\doublespacing

% This lets me use the normal operator, plus a whole bunch of other fun maths symbols 
\usepackage{amssymb}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{dsfont}
\usepackage{mathtools}

\usepackage{cite}

% This is for underlining in tables
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}

\begin{document}
\maketitle

\begin{abstract}
hi
\end{abstract}

\singlespacing
\pagestyle{plain}
\tableofcontents
\doublespacing

\pagebreak

\chapter{Introduction}
Everyone knows you write the intro last....



\section{SIGACTS Data}
This report examines the significant actions, or SIGACTS, log from the Afghanistan War released as part of wikileaks (ref). This records the type, time and place of every registered incident it should be noted that this will represent only a subset of the deaths which occurred throughout the period. Specifically this report focuses on two data streams - the direct and indirect fire rates. 

These take include both friendly and enemy action, direct fire is when a line of sight is available to the shooter blah.

\begin{figure}
\centering
\includegraphics{plotdata.png}
%[width=17cm]
\caption{Direct and Indirect incidents by day}
\label{fig:sigactsdata}
\end{figure}

\section{Nonhomogenous Poisson Processes}



\section{Non-Parametric Regression}

Regression is the process of determining the relationship f between inputs x and noisy output y; which is assumed to contain both a deterministic component f(x) and a stochastic one.

Given a set of observations D dihfshfs compute the posterior p(f | D) 


...

If the system dynamics are well known we could chose to fit with a parametric model and use the data to optimise the parameters. The problem with these models is their limited expressivity, we must be select an order for our polynomial up front and there is no guarantee that it will be able to fit the data appropriately.

Non-Parametric models have an expressivity which grows with the number of observations, meaning it is always possible for the resulting function to go through all of the training data (although this may not be appropriate). 


Allow us to say something about the function eg.smooth-ness

\chapter{Literature Review}
I can't be bothered to do this now.

\section{Gaussian Processes}

\section{LGCPs}

\section{}

\chapter{Gaussian Processes}

While a probability distribution describes the properties of variables, a stochastic process governs the properties of functions. \\
% ^ I think this should go earlier in the report....

A Gaussian Process (hereafter referred to as a GP) is a distribution on the functions \(\mathbf{f}\) where any finite subset of function values are distributed multivariate gaussian. 
%It can be thought of as a generalisation of the multivariate gaussian distribution to a potentially infinite number of variables.

A GP is completely defined by it's mean and covariance functions \(\mu (x)\) and \( k(x,x')\). 

There are an infinite number of possible functions which pass through all of the training data points, the form of the resulting prediction is controlled by the choice of mean and covariance functions \(\mu (x)\) and \( k(x,x')\), which completely define the GP.

\section{Mean and Covariance Function}

Sensible choices for mean and covariance function are paramount in a model's success. The chosen functions may contain unknown parameters, to be marginalised later, however if the form of the equations is not suitable a good fit is impossible. \\ \par
%\subsection{Mean Functions}

The mean function represents the form a process is expected to take in the absence of any data, meaning it dictates the predictions in areas far away from any observations.

In the case of complete ignorance a zero mean function is often assumed; expressing equal likelihoods of positive and negative relationships. 

This is not suitable for this data set as a negative number of incidents is not possible. Instead a constant mean function will be assumed, such as \ref{eq:GPmean}, where the \(\theta_1\) is a hyper-parameter left to be determined.

\begin{equation} \label{eq:GPmean}
\mu (x) = \theta_1
\end{equation}

\par
%\subsection{Covariance Functions}

The covariance function dictates the strength of relationship between any two input points to the GP. The covariance function \(k\) is used to assemble the covariance matrix \(\mathbf{K}\) so that: 

\begin{equation}
\mathbf{K(a,b)} =  \left( \begin{array}{cccc}
k(a_1,b_1) & k(a_1,b_2) &  \dots & k(a_1,b_m) \\
k(a_2,b_1) & k(a_2,b_2) &  \dots & k(a_2,b_m) \\
: & : & : & : \\
k(a_n,b_1) & k(a_n,b_2) &  \dots & k(a_n,b_m)  \end{array} \right) 
\end{equation}

In order for \(\mathbf{K}\) to be a valid covariance matrix it needs to be positive semi-definite, meaning it satisfies \( \mathbf{v^{T} K v} > 0\) for any \( \mathbf{v} \neq 0 \)

In practice, this requirement is satisfied by selected from a library of pre-existing valid covariance functions. Perhaps the most common of these, and the one which this report will focus on, is the squared exponential function.

\begin{equation}
k(x_1,x_2) = h^2 exp(- \frac{(x_1-x_2)^2}{2 l^2})
\end{equation} 

This is a function of the euclidean distance between the input points and two hyper-parameters: \(h\), the output scale, which controls the amplitude of the covariance and \(l\), the length scale, which dictates the range of input values deemed to be correlated. 

\section{Regression}

One of the main attractions of the Gaussian process framework is its computational tractability. Given a set of observations \( \mathbf{y}( \mathbf{x} ) = [y_1, y_2, ... y_n] \) which correspond to the known input values \( \mathbf{x} = [x_1, x_2, .. x_n] \) we can predict the distribution for novel input vector \( \mathbf{x_*} \) as \( y( \mathbf{x_*}) \sim \mathcal{N}(\mathbf{m_*,C_*}) \) \cite{GP-robots}. Equations \ref{eq:GPmean} and \ref{eq:GPvar} can be used to find this mean and covariance, where \(\mu\) is the chosen mean function and \(\mathbf{K}\) is the assembled covariance matrix.

\singlespacing

\begin{equation} \ref{eq:GPmean}
\mathbf{m_* = \mu (x_*) + K(x_* ,x) K(x,x)^{-1} (y(x) - \mu (x))}
\end{equation}

\begin{equation} \ref{eq:GPvar}
\mathbf{ C_* = K(x_*,x_*)-K(x_*,x) K(x,x)^{-1} K(x_*,x)}^{T}
\end{equation}

\doublespacing

Therefore a prediction can be produced for any \(\mathbf{y(x_*)}\) given only a set of observations \(\mathbf{y(x)}\). The predicted mean will go through all of the training data points, and zero variance will be predicted at these points. This is only appropriate if there is zero uncertainty in the observed data, which is often not the case.

\subsection{Observational Noise}

The SIGACTs data can be considered to be noise corrupted; inevitably some incidents will not have been recorded, and there may have been some human error in the data entry. 

Consequently it is not appropriate to assume that the training data points are known with complete confidence. This uncertainty can be incorporated by adding an unknown noise to predictions, so that the equations become \ref{eq:GP+noisemean} and \ref{eq:GP+noisevar} where \( \sigma^2 \) is the variance of the observed noise.

\singlespacing


\begin{equation} \label{eq:GP+noisemean}
\mathbf{m_*} = \boldsymbol{\mu} \mathbf{(x_*) + K(x_* ,x) (K(x,x)}+\sigma^2 \mathbf{I)^{-1} (y(x)} - \boldsymbol{\mu} \mathbf{(x))}
\end{equation}

\begin{equation} \label{eq:GP+noisevar}
\mathbf{ C_* = K(x_*,x_*)-K(x_*,x) (K(x,x)}+\sigma^2 \mathbf{I)^{-1} K(x_*,x)}^{T}
\end{equation}

\doublespacing

The resulting predicted mean will no longer necessarily go through all of the training data, and there will be some predictive uncertainty at these points.

\section{Learning Hyper-Parameters}
Using the suggested mean and covariance functions leaves 4 hyper-parameters which need to be chosen in order to produce a prediction. These are: \(\theta_1\) the mean value, \(l\) the length scale, \(h^2\) the output scale and \(\sigma^2\) the noise variance. 

The likelihood of a set of observations given a model is just equal to the multivariate Gaussian likelihood, given in \ref{eq:GPlikelihood}, where the \(p\)-length vector \(\mathbf{x}\) is the training data and the hyper-parameters are stored in a vector \(\mathbf{\theta}\).

\begin{equation} \label{eq:GPlikelihood}
L(\mathbf{x | \theta}) = \frac{1}{(2\pi)^{p/2} |\boldsymbol{\Sigma|}^{\frac{1}{2}}} exp(- \frac{1}{2} \mathbf{(x}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}))
\end{equation}

The maximum likelihood estimate for the hyper-parameters \(\mathbf{\theta}\) is the vector which maximises this quantity. Given the presence of the exponent it is easier computationally to consider maximising the log-likelihood, given in equation \ref{eq:GPloglikelihood}. This is valid because \(log\) is a strictly monotonically increasing function, meaning the locations of turning points are invariant under the transformation. \par
As the first term of the log-likelihood is independent of all the hyper-parameters, the problem of finding the MLE for \(\boldsymbol{\theta}\) can be described by equation \ref{eq:GPfmin}.

\singlespacing

\begin{equation} \label{eq:GPloglikelihood}
ln(L(\mathbf{x} | \boldsymbol{\theta})) = - \frac{p}{2} ln(2\pi) - \frac{1}{2} ln(|\boldsymbol{\Sigma}|) - \frac{1}{2} (\mathbf{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})
\end{equation}

\begin{equation} \label{eq:GPfmin}
\hat{\boldsymbol{\theta}} = \argmin_\theta{\{ln(|\boldsymbol{\Sigma}|) +(\mathbf{x}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}\}
\end{equation}

\doublespacing

As dependencies between the direct and indirect fire incidents has not been considered, there will be a set of hyper-parameters for each of the data streams. \par
% I hate everything not sure i should write about grid search
Optimization algorithms can compute the local minima given a function and a starting point, but it is difficult to determine when a global minima has been found. Given that the problem is in \(\mathds{R}^4\), a reasonable result should be obtained by using a grid search. This is where the optimiser is run repeatedly on using a \textit{grid} of starting points which span a range of each hyper-parameter. The lowest scoring result of these optimisations is assumed to be the global minimum. \par

\section{Results}

Once the hyper-parameters have been chosen, the mean vector and covariance matrix of the GP for a denser set of times can then be generated. The model resulting from a training data set of 100 sample points is shown in figure \ref{fig:GPresults}. The continuous line represents the function mean and the shaded area either side shows \(\pm\) 2 standard deviations, this means ideally 96\% of the data should be inside this area. The top images plot the training data points, while the bottom ones superimpose the entire data set (the testing data) over the predictions. The hyper-parameters selected for this model are listed in table \ref{GPhyperparameters}.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=8.5cm]{GPresultsd.png}
  	\caption{Direct Fire}
  	\label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  	\centering
  	\includegraphics[width=8.5cm]{GPresultsi.png}
  	\caption{Indirect Fire}
 	 \label{fig:sub2}
\end{subfigure}
\caption{Gaussian process models from 100 sampled data points}
\label{fig:GPresults}
\end{figure}

\singlespacing
\begin{table}[]
\centering
\caption{Maximum likelihood estimates for the GP hyper-parameters}
\label{GPhyperparameters}
\begin{tabular}{c|c|c|c|c|}
\cline{2-5}
\textbf{}                                    & \(\mu\) & \(h\) & \(l\) & \(\sigma^2\) \\ \hline
\multicolumn{1}{|c|}{\textbf{Direct Fire}}   & 14.01           & 9.56          & 102.12          & 28.01              \\ \hline
\multicolumn{1}{|c|}{\textbf{Indirect Fire}} & 6.64           & 3.47          & 87.42          & 10.66              \\ \hline
\end{tabular}
\end{table}
\doublespacing

While the majority of the data set is within the 2 standard deviation points, this is representative of the large levels of uncertainty predicted at all points in the model. More alarmingly, a significant amount of probability mass is placed on negative values - while the actual number of incidents in a day is constrained to be both positive and integer. \par 

% Also something about the direct failing to capture some characteristics near the end of the period

A poisson point process would be a more natural model for this data as this would only put weight on positive integer numbers. These are defined by a single parameter \(\lambda\), if this is allowed to vary with time the process is labelled non-homogenous. 

%Some bridge to the next chapter

\chapter{Log Cox Gaussian Processes}

% Poisson processes

A Cox process is essentially a non-homogenous Poisson point process where the underlying intensity \(\lambda\) is itself a stochastic process. In a Log Cox Gaussian process (LGCP) we assume that the log-intensity of the point process is a GP. 

\begin{equation} \label{eq:LGCPsetup}
\mathbf{v} = log(\boldsymbol{\lambda}) = \mathcal{G}\mathcal{P} \{ \mu(. ;\boldsymbol{\theta}) , \mathbf{K}(. , . ,\boldsymbol{\theta})\}
\end{equation}

This retains the expressivity of GPs while constraining the predicted intensities to be positive. However, the intensity values act as parameters meaning the optimisation problem becomes significantly harder. To reduce the complexity, one of the hyper-parameters can be eliminated by selecting the mean function in \ref{eq:LGCPmean}, with the caveat that we take \(log(0)=0\). This selects the average observed log-intensity as the constant mean. 

\begin{equation} \label{eq:LGCPmean}
\mu (x) = \frac{1}{N} \sum_{i=1}^{N} log_e(y_i)
\end{equation}

The underlying characteristics of \(\mathbf{v}\) should be similar to the GP regression case, so the same form of covariance function can be used - although the optimum hyper-parameters are likely to be significantly different.

\section{Regression}

The regression problem is more complicated; as well as the mean and covariance hyper-parameters, the discrete intensity values \(\mathbf{v}\) also need to be found. The training data \(\mathbf{D} = \{y_1,y_2, ..., y_N\}\) is composed of the number of observed incidents for a sample set of days. Unlike GPs, LGCPs are not capable of producing predictions for any input vector, but only for inputs corresponding to the observations. The resulting values of \(\mathbf{v}\) will then form the predicted mean of the model. \par

Bayes rule can be used to formulate the posterior for \(\mathbf{v}\), given in \ref{eq:LCGPposterior}, which optimal values of \(v_i\) will maximise. This expression requires a prior distribution of the hyper-parameters \(p(\boldsymbol{\theta})\) which can be approximated as a delta function at the \textit{true} parameters \(\hat{\boldsymbol{\theta}}\) reducing the expression to \ref{eq:LCGPposterior2}.

\singlespacing

\begin{equation} \label{eq:LCGPposterior}
p(\mathbf{v | D}) = \frac{\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta})p(\boldsymbol{\theta}) d\boldsymbol{\theta}}}{\int{\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta})p(\boldsymbol{\theta}) d\boldsymbol{\theta} d\mathbf{v}}}}
\end{equation}

\begin{equation}
p(\boldsymbol{\theta}) = \delta(\boldsymbol{\theta} - \hat{\boldsymbol{\theta}})
\end{equation}

\begin{equation} \label{eq:LCGPposterior2}
p(\mathbf{v | D}) = \frac{p(\mathbf{D|v})p(\mathbf{v}|\hat{\boldsymbol{\theta}}) }{\int{p(\mathbf{D|v})p(\mathbf{v}|\hat{\boldsymbol{\theta}}) d\mathbf{v}}}
\end{equation}

\doublespacing

For a point process model, the probability distribution for each observation \(y_i\) is Poisson with intensity \(e^{v_i}\). Assuming that observations are independent the likelihood \(p(\mathbf{D|v})\) is just the product of these poisson distributions, as expressed in \ref{eq:LGCPlikelihood}. As \(\mathbf{v}\) is a GP, the distribution given a set of hyper-parameters is just a multivariate Guassian likelihood. 

\singlespacing

\begin{equation} \label{eq:LGCPlikelihood}
p(\mathbf{D|v}) = \prod_{i=1}^{m} \mathcal{P}_o (y_i, e^{v_i})
\end{equation}

\begin{equation}
p(\mathbf{v}|\hat{\boldsymbol{\theta}}) = \frac{1}{\sqrt{(2\pi)^{m} |\hat{\boldsymbol{\Sigma|}}}} exp(- \frac{1}{2} \mathbf{(v}-\hat{\boldsymbol{\mu}})^{T}\hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{v}-\hat{\boldsymbol{\mu}}))
\end{equation}

\doublespacing

The denominator of \ref{eq:LCGPposterior} is intractable, so an approximation is needed in order to compute the posterior. This report focuses on Laplace's approximation, where the integrand \(P(\mathbf{x})\) is assumed to be gaussian and maximised at \(\mathbf{x=x_0}\). Equations \ref{eq:laplaceaprrox} and \ref{eq:laplaceaprrox2} give the form of the approximation. \cite{Mackay} When applied to the expression \(\int{\{p(\mathbf{D|v}) p(\mathbf{v}|\hat{\boldsymbol{\theta}})\} d\mathbf{v}} \) the result is only evaluated at the maximum \(\hat{\mathbf{v}}\) and therefore independent of the values \(v_i\). Therefore the maxima of the posterior and the product \(p(\mathbf{D|v}) p(\mathbf{v}|\hat{\boldsymbol{\theta}})\) are at the same location.

\singlespacing

\begin{equation} \label{eq:laplaceaprrox}
\int{P(\mathbf{x}) dx} \approx P(\mathbf{x_0}) \sqrt{\frac{2\pi}{c}}
\end{equation} 

\begin{equation} \label{eq:laplaceaprrox2}
c = - \frac{\partial^2}{\partial \mathbf{x}^2} ln P(\mathbf{x_0}) |_{\mathbf{x}=\mathbf{x_0}}
\end{equation}

\doublespacing 

We can concatenate the GP hyper-parameters and the log-intensity values into a single vector \( \boldsymbol{\gamma} = [\boldsymbol{\theta}, \mathbf{v}]^{T}\) which we can optimise over. Exploiting the monotonically increasing property of the \(log_e\) function, the problem can be written as:

\begin{equation} \label{eq:GPfmin}
\hat{\boldsymbol{\gamma}} = \argmin_\gamma{\{ \sum_{i=1}^{N}e^{v_i} - \mathbf{v}^{T}\mathbf{y} + \frac{1}{2}ln(|\boldsymbol{\Sigma}|) + \frac{1}{2}(\mathbf{v}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{v}-\boldsymbol{\mu})\}}
\end{equation}

The full posterior distribution \(p(\mathbf{v|D},\boldsymbol{\theta})\) can be approximated by the multivariate normal distribution in \ref{}. Where \(\boldsymbol{\mathcal{H}}\) is the hessian of the log distribution calculated at \(\hat{\mathbf{v}}\) 

\singlespacing
\begin{equation}
p(\mathbf{v|D},\boldsymbol{\theta}) \approx \mathcal{N} (\mathbf{v}; \hat{\mathbf{v}}, \boldsymbol{\mathcal{H}}^{-1})
\end{equation}

\begin{equation}
\boldsymbol{\mathcal{H}} = -\nabla^2 log p(\mathbf{v|D},\boldsymbol{\theta}) |_{\mathbf{v}=\hat{\mathbf{v}}}
\end{equation}
\doublespacing
% variance

\subsection{Reducing Complexity}

The optimization is now taking place in \(\mathds{R}^{N+3}\), where N is the number of training points used. This large increase in space means that a grid search is no longer appropriate, the computation time would be infeasible. As our predictions occur at the same times as the observations, it is reasonable to assume that the optimal values \(v_i\) will be close to the log-observed values \(log_e(y_i)\). This means the complexity of the search can be reduced by always starting the optimiser at \ref{eq:LGCPstartpoint}, meaning only the 3 hyper-parameters in \(\boldsymbol{\theta_0}\) need to be varied over. \par

\begin{equation} \label{eq:LGCPstartpoint}
\boldsymbol{\gamma_0} = \left( \begin{array}{cc}
\boldsymbol{\theta_0} \\
log_e(\mathbf{y}) \end{array} \right) 
\end{equation}

The number of optimization calculations can be reduced by using a latin hypercube, which given a range for each of the parameters randomly selects start points which are dissimilar. This reduces the chance of multiple start points leading to the same local minimum. \par

The tolerance used by an optimiser strongly influences the time-cost of the operation, this is the maximum change in function value between iterations at which the algorithm will stop. Many of the start points selected by the latin hypercube will be obviously sub-optimal, meaning it is wasteful to perform accurate optimization at these points. \par

This problem can be avoided by performing an initial round of optimization with a large function tolerance, which acts to identify the rough location of the global minimum. The output from this can be used as the start point for a single more precise optimization, where the local minimum found can reasonably be assumed to be global.

%cholesky decomposition

\subsection{Initial Results}

The initial predictions produced with 60 sampled training points are shown in figure \ref{fig:LGCPresults}. The smaller training set was necessitated in order to reduce computation time, but otherwise the format is the same as for the GP results. \par

All probability mass is assigned to positive numbers, and the predicted variances are much smaller - meaning the level of uncertainty has been reduced. A preliminary look at the testing plots suggests that the majority of the observed  data lies within the predicted \(\pm\) 2 standard deviation area. However, the training data shows a worrying number of points outside of this region - far more than the ideal 4\% which could suggests this model is not a good fit. 

\par
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=8.5cm]{LGCPresultsd.png}
  	\caption{Direct Fire}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  	\centering
  	\includegraphics[width=8.5cm]{LGCPresultsi.png}
  	\caption{Indirect Fire}
\end{subfigure}
\caption{Log Gaussian Cox process models from 60 sampled data points}
\label{fig:LGCPresults}
\end{figure}

The hyper-parameters learned are listed in Table \ref{LGCPhyperparameters}. Similar length scales have been chosen for the data streams, which is unsurprising as changes to direct fire rates are likely to be closely related to those in indirect fire rates. 

%%%%%

\singlespacing
\begin{table}[]
\centering
\caption{Estimates for the LGCP hyper-parameters}
\label{LGCPhyperparameters}
\begin{tabular}{c|c|c|c|}
\cline{2-4}
\textbf{}                                    & \(h\) & \(l\) & \(\sigma^2\) \\ \hline
\multicolumn{1}{|c|}{\textbf{Direct Fire}}   & 0.8068          & 63.78          & \(1.304\times 10^{-8}\)              \\ \hline
\multicolumn{1}{|c|}{\textbf{Indirect Fire}} & 0.4098          & 61.12          & \(1.5672\times 10^{-7}\)              \\ \hline
\end{tabular}
\end{table}
\doublespacing

\section{Including Correlation}

% Change this bit to include Mike's paper that i found

Given the probable correlation between quantities it seems counterintuitive to construct two independent models for the data streams; not only will the hyper-parameters likely be similar, but if the events are correlated then the training data for one stream can provide valuable information for modelling the other.\par

This section explores a single model, which takes as training data samples of both the direct and indirect fire rates and outputs predictions for both using the approach outlined in \cite{multi-outputGP}.

\subsection{Model Adaptation}

The LGCP can be easily adapted to fit a multi-input, multi-output model. The training data streams are concatenated into a single vector \ref{eq:LGCPCy}, and a step function of the form \ref{eq:LGCPCmean} can be assumed for the mean function. The first half of resulting predictions for \(\mathbf{v}\) will be for direct incidents, and the second for indirect.

\begin{equation} \label{eq:LGCPCy}
\mathbf{y} = \left( \begin{array}{cc}
\mathbf{y^{direct}} \\
\mathbf{y^{indirect}} \end{array} \right)
\end{equation}

\begin{equation} \label{eq:LGCPCmean}
\mu (x) = \begin{cases}  
\displaystyle \sum_{i=1}^{N} log_e(y_i^{direct}) & for \text{ }x\le N \\
\displaystyle \sum_{i=1}^{N}log_e(y_i^{indirect}) & for \text{ }x>N \\ \end{cases}
\end{equation}

The covariance function now takes four inputs: the time inputs \(x_1, x_2\) and the labels \(l_1,l_2\) which denote which data series the input value belongs to. For two points to be highly correlated they need to be both close in distance and the same type of incident so it is appropriate to model \(k\) as a product \ref{eq:LGCPCcov},where \(k_{SE}\) is the squared exponential function with unity variance and \(K_L\) is a square matrix whose size is equal to the number of data streams - in this case two. 

\begin{equation} \label{eq:LGCPCcov}
k(x_1,x_2,l_1,l_2) = k_{SE}(x_1,x_2) \text{ } K_L(l_1,l_2) 
\end{equation}

The squared exponential function \(k_{SE}\) is still appropriate for modelling the time component. Both the direct and indirect fire rates must have the same length scale in order for the covariance matrix to be positive semi-definite, but they may have different output scales and noise variances.

Equation \ref{eq:KL-decomp} shows how the matrix \(K_L\) can be decomposed into a vector \(\boldsymbol{\lambda}\) which contains the respective output scales of the data streams and a matrix \(\mathbf{S}\) which models the correlation between data streams. The diagonal elements o f \(\mathbf{S}\) need to be 1 as a data stream is assumed to be perfectly correlated with itself, and all entries should be \(\in [0,1]\).

\singlespacing
\begin{equation} \label{eq:KL-decomp}
\begin{aligned}
K_L =& \boldsymbol{\lambda}^T \mathbf{S} \boldsymbol{\lambda} \\ \\
\boldsymbol{\lambda} = \left( \begin{array}{cc}
\rho_1  \\
\rho_2 \end{array} \right)\text{ } & \text{ }
\mathbf{S} = \left( \begin{array}{cc}
1 & \alpha  \\
\alpha  & 1 \end{array} \right)
\end{aligned}
\end{equation}
\doublespacing

The complete covariance matrix is given in \ref{eq:LGCPCcovariance} where \(\rho_1\) , \(\rho_2\) are the respective output scales, and \(\sigma_d^2\) , \(\sigma_i^2\) the noise variances.

\begin{equation} \label{eq:LGCPCcovariance}
\begin{aligned}
\boldsymbol{\Sigma} =&  \left( \begin{array}{cc}
\rho_1^2 \mathbf{K} + \sigma_i^2 \mathbf{I} & \alpha \rho_1 \rho_2 \mathbf{K}  \\
\alpha \rho_1 \rho_2 \mathbf{K} & \rho_2^2 \mathbf{K} + \sigma_d^2 \mathbf{I} \end{array} \right) \\ where\\
&\mathbf{K}(i,j) = exp\left( \frac{(x_i-x_j)^2}{l^2}\right)
\end{aligned}
\end{equation}

There are now 6 hyper-parameters to be found, which are summarised in Table \ref{LGCPChyperparameters}. For the novel correlation coefficient it may not be necessary to try multiple dissimilar start points, as empirical methods can provide a reasonable estimate of the most likely value.

Multivariate normal distributions allow simple analysis of the correlation between two randomly distributed variables. If the joint distribution of the direct and indirect fire incidents can be expressed as a 2-D gaussian, as in \ref{eq:2dmvg}, then the correlation coefficient can be approximated as \ref{eq:2dcorrelation}.

Maximum likelihood can be used to find the best fitting 2D covariance matrix across the entire data set, the result of which is shown in \ref{fig:correlation}

\begin{equation} \label{eq:2dmvg}
\mathcal{N} \left( \left[ \begin{array}{cc}
\mu_x \\
\mu_y \end{array} \right], \left[ \begin{array}{cc}
\sigma_{x}^2 & \sigma_{xy} \\
\sigma_{xy} & \sigma_{y}^2  \end{array} \right] \right)
\end{equation}

\begin{equation} \label{eq:2dcorrelation}
\alpha = \frac{\sigma_{xy}}{\sqrt{\sigma_{x}^2 \sigma_{y}^2}}
\end{equation}

\begin{figure}
\centering
\includegraphics[width=12cm]{direct_indirect_correlation.png}
\caption{caption me}
\label{fig:correlation}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=12cm]{direct_indirect_correlation2.png}
\caption{caption me}
\label{fig:lcorrelation}
\end{figure}


The optimisation problem is now in \(\mathds{R}^{2N+6}\), meaning the size of the training data used needs to be kept relatively low to avoid excessive computation time. If the training set is too small, there is a risk the model will not capture all features of the data. To ensure this isn't happening regression can be performed on multiple independent training sets, if consistent models are predicted then the data set was large enough.

% ^ SHOULD I DO SOMETHING ACTUALLY ABOUT IT?

\begin{table}[]
\centering
\caption{Hyperparameters of the multi-input LGCP model}
\label{LGCPChyperparameters}
\begin{tabular}{ll|c}
\multicolumn{2}{c|}{\textbf{Hyperparameter}} & \textbf{Constraint} \\ \hline
\(\rho_1\)           & Direct output scale             & \(\geq0\)            \\
\(\rho_1\)             & Indirect output scale           & \(\geq0\)            \\
\(l\)          & Length scale                    & \(\geq0\)            \\
\(\alpha\)             & Correlation                     &  \(\in\{-1,1\}\)                   \\
\(\sigma_d\)             & Direct noise variance           &    \(\geq0\)                 \\
\(\sigma_i\)           & Indirect noise variance         &     \(\geq0\)               
\end{tabular}
\end{table}

\subsection{Results}
The predictions produced for a training sample of 70 points are shown in Figure \ref{fig:LGCPCresults}. The blue line represents the predicted mean of the direct data and the green the indirect, similarly the shaded areas represent the respective \(\pm\) 2 std bounds. The top plot shows includes the training data points, while the bottom superimposes the predictions over the full data sets. \par

\begin{figure}
\centering
\includegraphics[width=13cm]{LGCPCresults.png}
\caption{caption me}
\label{fig:LGCPCresults}
\end{figure}

\begin{table}[]
\centering
\caption{Optimum hyperparameters for multi-input LGCP model}
\label{LGCPChyper}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\(\rho_1\)} & \multicolumn{1}{c|}{\(\rho_2\)} & \(l\)                      & \(\sigma_d^2\)            & \(\sigma_i^2\)            & \(\alpha\) \\ \hline
0.4170                            & 0.8825                           & \multicolumn{1}{c|}{41.643} & \(3.9100 \times 10^{-4}\) & \(3.3323 \times 10^{-4}\) & 0.5772     \\ \hline
\end{tabular}
\end{table}

%%%%!!!
Something about what the predictions look like. 

\chapter{Changepoints}

So far this report has focused on finding a single of hyper-parameters which can accurately describe the entire dataset. However, as the period spans four years it's likely that the system dynamics change at some point during the conflict.



Changepoints can be incorporated into regression models at the expense of added hyperparameters, and hence complexity.

\section{Sudden Change in Output Scale}

The simplest case is a step change in only one of the hyper parameters, as considered in \cite{changepoint-prediction}. As the SIGACTs data shows a sharp increase in direct fire incidents towards the end of the period, a sudden drastic change in output scale may be an appropriate model.

This can be achieved by using the altered covariance function in \ref{eq:outputcpcov} where \(k_{se}\) is the squared exponential kernel with unity output scale and \(K_L\) is the correlation matrix from the previous model and \(a\) is defined in \ref{eq:outputcpcov2}. This function increases the output scales of both data series by a factor of \(c\) after some point \(x_c\), as it is a purely injective function the resulting covariance remains positive definite. 

\singlespacing
\begin{equation} \label{eq:outputcpcov}
k(x_1,x_2,l_1,l_2) = a(x_1,x_2)\text{ } K_L(l_1,l_2) \text{ }k_{se}(x_1,x_2)
\end{equation}

\begin{equation} \label{eq:outputcpcov2}
a(x_1,x_2) = \begin{cases}
1 & for \text{ } x_1, x_2 \leq x_c \\
c & for \text{ } x_1, x_2 \geq x_c \\ 
\sqrt{c} & otherwise \\ 
\end{cases}
\end{equation}
\doublespacing

Two new hyperparameters have been introduced: the scale factor \(c\) and the changepoint location \(x_c\) which, as an index, must be integer. 

In order to asses the suitability of this model it's useful to look at the variation of model evidence with changepoint location; if the sudden change in output scale model is suitable there should be a sharp peak at one location and every other position should be comparably unfavourable. 

The results for the SIGACTSs data is shown in figure \ref{fig:oscpvariation}. This shows that a relatively similar probability is assigned to every changepoint location, suggesting that a sudden change in output scale model is not a good fit for the data. 

\begin{figure}
\centering
\includegraphics[width=12cm]{oscpvariation.png}
\caption{Variation of likelihood with Changepoint location}
\label{fig:oscpvariation}
\end{figure}

\section{Sudden Change in Correlation}

For a change in correlation a different approach is needed in order to ensure a positive definite covariance.  

Instead of modelling the two data streams directly it makes sense to model the indirect fire intensities and then the difference between the two. This is equivalent to expressing the log intensities \(v_d\) and \(v_i\) as a linear combination of two independent functions \(f\) and \(g\) which are defined in \ref{eq:ccpfandg}.

\begin{equation} 
\left[ \begin{array}{cc}
v_{d}(x)  \\
v_{i}(x) \end{array} \right] = \left[ \begin{array}{cc}
1 & 1  \\
1  & 0 \end{array} \right] \left[ \begin{array}{cc}
f(x)  \\
g(x) \end{array} \right]
\end{equation}

\begin{equation} \label{eq:ccpfandg}
\left[ \begin{array}{cc}
f(x)  \\
g(x) \end{array} \right] \sim \mathcal{G}\mathcal{P} \left( \left[ \begin{array}{cc}
\mu_i  \\
\mu_d - \mu_i \end{array} \right], \left[ \begin{array}{cc}
K_f & 0  \\
0  & K_g \end{array} \right] \right)
\end{equation}

The covariance of the indirect log-intensities \(K_f\) can take the same form used earlier in this report, where LGCPs were used to model the data streams independently, so that the covariance function is given by \ref{eq:kf} containing two hyper-parameters: the length and output scales. 

\begin{equation} \label{eq:kf}
k_f(x_1,x_2) = \rho_1 \text{ }exp\{\frac{-(x_1-x_2)^2}{2 l_1^2}\}
\end{equation}

Modelling the difference function \(g\) directly allows much more complex analysis of the change in relationship between the two data streams, which appears to change throughout the period. Therefore it is the covariance \(K_g\) which should capture the changepoint. As there is an apparent increase in the variation between the data stream the change could be modelled by an increase in the output scale. The data is still a time-series so a squared exponential covariance is still appropriate therefore \ref{eq:kg} is used, where \(a\) has the same form as in \ref{eq:outputcpcov2}.

\begin{equation} \label{eq:kg}
k_g(x_1,x_2) = \rho_2 \text{ }exp\{\frac{(x_1-x_2)^2}{2 l_2^2}\} \text{ } a(x_1,x_2)
\end{equation}


As the training data consists of observations of direct and indirect incidents, the mean vector and covariance matrix need to be transformed into this form. For a transformation of the form \(\mathbf{y}= \mathbf{M} \mathbf{x}\) these arrays are given by \(\boldsymbol{\mu_y}= \mathbf{M}\text{ }\boldsymbol{\mu_x}\) and \(\mathbf{cov_y}= \text{ }\mathbf{M}\text{ } \mathbf{cov_x} \text{ } \mathbf{M}^{T}\) (REF?) \par

The observations can also be assumed to be noise corrupted. In previous models maximum likelihood estimation has selected very similar values for the noise variance of the individual data streams. Therefore, in the interests of reducing the number of hyper-parameters, the noise on all observations can be modelled as the same. This means the log-intensities of the data streams can be modelled by the Gaussian process defined in \ref{eq:fandggp}

\begin{equation} \label{eq:fandggp}
\left[ \begin{array}{cc}
v_d(x)  \\
v_i(x) \end{array} \right] \sim \mathcal{G}\mathcal{P} \left( \left[ \begin{array}{cc}
\mu_d  \\
\mu_i \end{array} \right], \left[ \begin{array}{cc}
K_f + K_g & K_f  \\
K_f  & K_f  \end{array} \right] + \sigma^2 \mathbf{I} \right)
\end{equation}




\begin{figure}
\centering
\includegraphics[width=12cm]{ccpvariation.png}
\caption{Variation of likelihood with Changepoint location}
\label{fig:ccpvariation}
\end{figure}
\chapter{Testing}

% I need to add something about my in test model - i don't remember exactly what it was called which might be an issue .

It's useful to have a formal method of comparing the success of models

Given a testing data set composed of observations \(\mathbf{z_i}\) containing the number of direct and indirect fire rates on the \(i\)th day, the first task is to identify the model predictions at that time step.

It is important that the testing data set is independent of the training set in order to fairly analyse the model. As only a small percentage of the 

For the GP model this is trivial as predictions can be produced for an arbitrary set of inputs, so all integer days in the range can be forecasted meaning the mean and variance corresponding to the test observation needs just to be selected. 

Whereas, for the LGCP model predictions for the mean and variance are only generated at time steps corresponding to the training data. 

\section{Test Model}
grrr

\section{Evaluation Methods}
The mean square error is a crude but effective evaluation tool, which measures the square distance between the training data observations and the predicted mean at that time step and sums over all testing data. The calculation is given in \ref{eq:mse}, where an N-length set of testing data has been used containing observations \(y_i\) corresponding to known inputs \(x_i\). This would produce an index of the success of the model for a single data stream, meaning scores for both the direct and indirect incident data would be obtained in each case. 

\begin{equation} \label{eq:mse}
mse = \frac{1}{N} \sum_{i=1}^{N} (y_i - f(x_i))^2
\end{equation}

While this measure gives some idea of the accuracy of the predicted mean, no account is taken of the model's predicted variance. Clearly it's preferable that the model perform badly in an area where it has declared a large uncertainty than in places where the predicted variance is low. \par

%\section{I don't know what to call this}
% Not average likelihood but average log likelihood
A better solution is to approximate the likelihood of the testing data. 

For large testing data sets the likelihood may become to small to compute, as the observations are being considered independent we can instead find the average likelihood of a single data point.

\section{Comparison}

Given a testing data set of 500 points, the results of both evaluation methods for each model are listed in Table \ref{model-comparison}. \par

The units of the mean square error test are incidents\(^2\), so lower numbers are preferable as they signify that the testing points are closer to the predicted mean. Contrastingly 

\begin{table}[]
\centering
\caption{Formal evaluation of the 3 different models}
\label{model-comparison}
\begin{tabular}{ccccc}
\multicolumn{1}{l}{}                                       & \multicolumn{4}{c}{{\ul \textbf{Test}}}                                                                                                                                           \\
\multicolumn{1}{c|}{\multirow{2}{*}{{\ul \textbf{Model}}}} & \multicolumn{2}{c|}{\textbf{Mean Square Error}}                                         & \multicolumn{2}{c|}{\textbf{Average Likelihood}}                                                         \\
\multicolumn{1}{c|}{}                                      & \multicolumn{1}{c|}{\textit{Direct Fire}} & \multicolumn{1}{c|}{\textit{Indirect Fire}} & \multicolumn{1}{c|}{\textit{Direct Fire}} & \multicolumn{1}{c|}{\textit{Indirect Fire}} \\ \hline
\multicolumn{1}{|c|}{\textbf{GP Model}}                    & \multicolumn{1}{c|}{29.1917}              & \multicolumn{1}{c|}{11.5186}                & \multicolumn{1}{c|}{0.0424}               & \multicolumn{1}{c|}{0.0691}                 \\ \hline
\multicolumn{1}{|c|}{\textbf{Basic LGCP Model}}            & \multicolumn{1}{c|}{32.9365}              & \multicolumn{1}{c|}{15.4097}                & \multicolumn{1}{c|}{0.0476}               & \multicolumn{1}{c|}{0.0134}                 \\ \hline
\multicolumn{1}{|c|}{\textbf{LGCP + Correlation Model}}    & \multicolumn{1}{c|}{25.5107}              & \multicolumn{1}{c|}{15.1999}                & \multicolumn{1}{c|}{0.0423}               & \multicolumn{1}{c|}{0.1093}                 \\ \hline
\end{tabular}
\end{table}




\chapter{Conclusion}
Not much of a report, but it's a start.

\singlespacing 

\bibliographystyle{plain}
\bibliography{report}

\end{document}
