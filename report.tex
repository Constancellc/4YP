\documentclass[a4paper,11pt]{report}

%\usepackage[margin=1in]{geometry}

% This sets the margins. They can be reduced to 20mm if i need the room later!
\usepackage{geometry}
 \geometry{
 a4paper,
 left=20mm,
 top=30mm,
 }


% In order to import matlab figures
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

% This sets the font to arial
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

% This changes the heading distances at the start of chapters
\usepackage{titlesec}
\titleformat{\chapter}[display]   
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{20pt}{\Huge}   
\titlespacing*{\chapter}{0pt}{10pt}{15pt}

% This stops a new page from being created
\usepackage{etoolbox}
\makeatletter
\patchcmd{\chapter}{\clearpage}{}{}{}
\makeatother


% This changes the form of the chapter title
\usepackage[T1]{fontenc}
\usepackage{titlesec, blindtext}
\newcommand{\hsp}{\hspace{20pt}}
\titleformat{\chapter}[hang]{\LARGE\bfseries}{\thechapter\hsp}{0pt}{\LARGE\bfseries}
\titleformat{\section}[hang]{\large\bfseries}{\thesection\hsp}{0pt}{\large\bfseries}
% For table and figure
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}



% This lets us use double spacing
\usepackage{setspace}
\doublespacing

% This lets me use the normal operator, plus a whole bunch of other fun maths symbols 
\usepackage{amssymb}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{dsfont}
\usepackage{mathtools}

\usepackage{cite}

% This is for underlining in tables
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}


\author{{\LARGE Constance Crozier}\\ Christ Church }
%Department of Engineering Science, \\ University of Oxford
\title{{\Huge \textbf{Bayesian Non-Parametrics for Data from the War in Afghanistan}}}
\date{May 2016}

\begin{document}
%\maketitle
\begin{titlepage}
	\centering
	\includegraphics[width=0.25\textwidth]{logo.png}\par\vspace{1cm}
	%{\scshape\LARGE Columbidae University \par}
	\vspace{1cm}
	{\Huge\bfseries Bayesian Non-Parametrics for Data from the War in Afghanistan\par}
	\vspace{2cm}
	{\Large Constance Crozier\par}
	\vspace{1cm}
	{\large Christ Church\par}
	\vfill
	{\small supervised by\par}
	Professor Michael \textsc{Osborne}

	\vfill

% Bottom of the page
	{\large May 2016\par}
\end{titlepage}



\begin{abstract}
This report examines the SIGACTS data, which records all significant actions between 2006 and 2010 in the Afghanistan war. It focuses on the direct and indirect Fire events recorded, and aims to perform Bayesian inference on these two data streams. 

Initially the data is analysed as a Gaussian Process, optimised using maximum likelihood estimation, before moving on to Log-Cox Gaussian Processes. The correlation between the Direct and Indirect fire incidents is initially explored by incorporating both data streams into a multi-output model. Apparent changes in this relationship leads to the investigation of changepoint models, particularly in finding a way to model a change in the correlation. 

\end{abstract}

\singlespacing
\pagestyle{plain}
\tableofcontents
\doublespacing

\pagebreak

\chapter{Introduction}
\section{Background}

In October 2001, in response to the 9/11 attacks, the United States invaded Afghanistan. Although initially supported by only close allies, including the United Kingdom, they were joined by NATO in 2003. Operation Enduring Freedom lasted until the 31st December 2014, and was estimated to have led to the deaths of at least 55,000 insurgents and 21,200 civilians.\cite{bodycount}

On the 25th July 2010 WikiLeaks released over 75,000 secret US military reports detailing events in Afghanistan from 2004 to 2010. \cite{wikileaks} The publication was intended to provide the public with a more comprehensive understanding of the reality of the war.

\section{SIGACTS Data}
This report examines the SIGACTS data, which forms the base of the 'Afghan War Diary'. This comprises all the significant actions logged by coalition forces from January 2006 until March 2010. The type, time and place of every registered incident is recorded; an example entry is shown in Table \ref{tab:sigactseg}. 

\begin{table}[]
\centering
\caption{Example log from the SIGACTs data}
\label{tab:sigactseg}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\bf{Date} & \bf{Category} & \bf{District} & \bf{Location} &  \bf{Province} & \bf{Region} & \bf{Type} \\ \hline
01/01/2006 & IED Found/Cleared & Zhari & 31.6x65.4 & Kandahar & RC SOUTH & Explosive Hazard \\ \hline
\end{tabular}
\end{table}

Specifically this report focuses on two specific 'categories' of event: the direct and indirect fire incidents. Direct fire is the label given to any attack in which a line of sight is available to the assailant, while indirect fire includes ballistic missiles and other weapons for which a line of sight is not required.

These were chosen due to the their abundance, providing a large and meaningful dataset, as well as their interesting relationship. The frequency of these events, binned by day, are shown in Figure \ref{fig:sigactsdata}. It should be noted that the y-axis has been restricted, concealing the number of direct and indirect fire events on day 1328, which are 204 and 182 respectively. This corresponds to 20th August 2009, the date of the Afghanistan elections; for the rest of the report this date shall be considered anomalous and removed from the data set.

\begin{figure}
\centering
\includegraphics{plot_data.png}
\caption{Direct and Indirect Fire incidents by day}
\label{fig:sigactsdata}
\end{figure}

\section{Modelling}

This report aims to use statistical techniques to model and analyse the two data streams throughout the period. The problem can be considered one of regression, the process of determining the relationship \(f\) between inputs \(\mathbf{x}\) and noisy outputs \(\mathbf{y}\). In this case \(\mathbf{y}\) will be a set of observations of the data at discrete times. A subset of the data set will be used to train the model, and the remaining data can be used to evaluate the success of a model. \par

If the system dynamics are well understood parametric modelling is the natural course of action, where a form of model (for example, quartic) is chosen and the data is used to optimise the parameters. However these models have a limited expressivity; if the form of model chosen is not appropriate a good fit is impossible. As there is no obvious choice of model in this case, parametric modelling will not be explored in this report. \par

Contrastingly, non-parametric models have an expressivity which grows with the number of observations. It is always possible for the resulting function to pass through all of the training data, although this may not be desirable. Although the form of the model is not fixed, users can specify properties that they believe the function to have, for example smoothness. \par

Chapter 3 of this report explores the use of Gaussian Processes to model the data, while Chapter 4 uses Log Cox Gaussian Processes. Chapter 5 investigates the presence of changepoints in the data. Generating predictions for unobserved inputs is covered in Chapter 6, and Chapter 7 presents methods of evaluating and comparing the success of models. 

\chapter{Literature Review}

\section{Gaussian / Log Gaussian / Cox Processes}
This report begins by focusing on Gaussian Processes, which are defined extensively in \textit{Rasmussen and Williams, 2006} \cite{GP4ML}. Their use in modelling time series data is introduced by \textit{Roberts et. al, 2012} in \cite{GP-robots}, and the idea of modelling multiple data streams with one process is covered by \textit{Osborne et. al, 2008} \cite{multi-outputGP}.

The concept of Log Cox Gaussian Processes are introduced by \textit{M\o ller et. al, 1998} \cite{LGCP-moller} and their use in spatial and spatio-temporal data is covered by \textit{Diggle et. al, 2013}\cite{LGCP-diggle}. Using Log Cox Gaussian Processes to make predictions in the presence of changepoints is discussed by \textit{Garnett et. al} in \cite{changepoint-prediction}.

\section{Changepoints}

In its later stages, this report investigates sudden changes in the dynamics of the the data streams, known as changepoints.  Analysis of Poisson processes with changepoints that are modelled as a step change in intensity are covered by \textit{Carlin et. al, 1992} \cite{carln-cp} and \textit{Raftery and Akman, 1986} \cite{akman-cp}.

\textit{Saatci et. al, 2010} specifically studies changepoints in the dynamics of Gaussian Processes \cite{saatci-cp}. However the results are not applied to non-stationary point processes and the focus is on the detection of changepoints, rather than making predictions in their presence.

\chapter{Gaussian Processes}

While a probability distribution describes the properties of variables, a stochastic process governs the properties of functions. \\

A Gaussian Process (hereafter referred to as a GP) is a distribution on the functions \(\mathbf{f}\) where any finite subset of function values are distributed multivariate Gaussian. 
%It can be thought of as a generalisation of the multivariate gaussian distribution to a potentially infinite number of variables.

There are an infinite number of possible functions which pass through all of the training data points; the form of the resulting prediction is controlled by the choice of mean and covariance functions \(\mu (x)\) and \( k(x,x')\), which completely define the GP. 
% where better than here below?
Here \(\mu (x)\) outputs the process mean at a point \(x\) and \( k(x,x')\) gives the covariance of the values of the process at the two locations \(x\) and \(x'\).

\section{Mean and Covariance Function}

Sensible choices for mean and covariance function are paramount to a model's success. If the form of the expressions chosen is ill-suited to the data a good fitting model is impossible. The chosen functions may contain unknown variables, which can be marginalised later, these are known as hyper-parameters. \\

The mean function represents the form a process is expected to take in the absence of any data, meaning it dictates the predictions in areas far away from any observations. In the case of complete ignorance a zero mean function is often assumed; expressing equal likelihoods of positive and negative relationships, and ensuring only zero-predictions are made far from the observed data. \par

This is not suitable for the data set being considered as a negative number of incidents is not possible. Instead a constant mean function will be assumed, so that

\begin{equation} \label{eq:GPmean}
\mu (x) = \theta_1
\end{equation}

where the \(\theta_1\) is a hyper-parameter left to be determined.

The covariance function dictates the strength of relationship between any two input points to the GP. The covariance function \(k\) is used to assemble the covariance matrix \(\mathbf{K}\) so that: 

\begin{equation}
\mathbf{K(a,b)} =  \left( \begin{array}{cccc}
k(a_1,b_1) & k(a_1,b_2) &  \dots & k(a_1,b_m) \\
k(a_2,b_1) & k(a_2,b_2) &  \dots & k(a_2,b_m) \\
: & : & : & : \\
k(a_n,b_1) & k(a_n,b_2) &  \dots & k(a_n,b_m)  \end{array} \right) 
\end{equation}

In order for \(\mathbf{K}\) to be a valid covariance matrix it needs to be positive semi-definite, meaning it satisfies \( \mathbf{v^{T} K v} \geq 0\) for any \( \mathbf{v} \neq 0 \).

In practice, this requirement is satisfied by selecting from a library of pre-existing valid covariance functions. Perhaps the most common of these, and the one which this report will focus on, is the squared exponential function:

\begin{equation}
k(x_1,x_2) = h^2 \exp^{- \frac{(x_1-x_2)^2}{2 l^2}}.
\end{equation} 

This is a function of the Euclidean distance between the input points and two hyper-parameters: \(h\), the output scale, which controls the amplitude of the covariance and \(l\), the length scale, which dictates the range of input values deemed to be correlated. 

\section{Regression}

One of the main attractions of the Gaussian process framework is its computational tractability. Given a set of observations \( \mathbf{y}( \mathbf{x} ) = [y_1, y_2, ... y_n] \) which correspond to the known input values \( \mathbf{x} = [x_1, x_2, .. x_n] \) we can predict the distribution for novel input vector \( \mathbf{x_*} \) as \( y( \mathbf{x_*}) \sim \mathcal{N}(\mathbf{m_*,C_*}) \) \cite{GP-robots}. Equations \ref{eq:GPmean} and \ref{eq:GPvar} can be used to find this mean and covariance, where \(\mu\) is the chosen mean function and \(\mathbf{K}\) is the assembled covariance matrix.

\singlespacing

\begin{equation} \label{eq:GPmean}
\mathbf{m_* = \mu (x_*) + K(x_* ,x) K(x,x)^{-1} (y(x) - \mu (x))}
\end{equation}

\begin{equation} \label{eq:GPvar}
\mathbf{C_* = K(x_*,x_*)-K(x_*,x) K(x,x)^{-1} K(x_*,x)}^{T}
\end{equation}

\doublespacing

Therefore a predicted distribution can be produced for any \(\mathbf{y(x_*)}\) given only a set of observations \(\mathbf{y(x)}\). The predicted mean will go through all of the training data points, and zero variance will be predicted at these points. This is only appropriate if there is zero uncertainty in the observed data, which is often not the case.

\subsection{Observational Noise}

The SIGACTs data can be considered to be noise corrupted; inevitably some incidents will not have been recorded, and there may have been some human error in the data entry. 

Consequently it is not appropriate to assume that the training data points are known with complete confidence. This uncertainty can be incorporated by adding an unknown noise parameter to predictions, so that the equations become \ref{eq:GP+noisemean} and \ref{eq:GP+noisevar} where \( \sigma^2 \) is the variance of the observed noise.

\singlespacing


\begin{equation} \label{eq:GP+noisemean}
\mathbf{m_*} = \boldsymbol{\mu} \mathbf{(x_*) + K(x_* ,x) (K(x,x)}+\sigma^2 \mathbf{I)^{-1} (y(x)} - \boldsymbol{\mu} \mathbf{(x))}
\end{equation}

\begin{equation} \label{eq:GP+noisevar}
\mathbf{ C_* = K(x_*,x_*)-K(x_*,x) (K(x,x)}+\sigma^2 \mathbf{I)^{-1} K(x_*,x)}^{T}
\end{equation}

\doublespacing

The resulting predicted mean will no longer necessarily go through all of the training data, and there will be some predicted uncertainty at these points.

\section{Learning Hyper-Parameters}
Using the suggested mean and covariance functions leaves 4 hyper-parameters which need to be chosen in order to produce a prediction. These are: \(\theta_1\) the mean value, \(l\) the length scale, \(h^2\) the output scale and \(\sigma^2\) the noise variance. 

The likelihood of a set of observations for a known model is given by the multivariate Gaussian likelihood:

\begin{equation} \label{eq:GPlikelihood}
L(\mathbf{x | \boldsymbol{\theta}}) = \frac{1}{(2\pi)^{p/2} |\boldsymbol{\Sigma|}^{\frac{1}{2}}} exp(- \frac{1}{2} \mathbf{(x}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}))
\end{equation}

where the \(p\)-length vector \(\mathbf{x}\) is the training data and the hyper-parameters are stored in a vector \(\boldsymbol{\theta}\).

The maximum likelihood estimate for the hyper-parameters \(\boldsymbol{\theta}\) is the vector which maximises \ref{eq:GPlikelihood}. Given the presence of the exponent it is easier computationally to consider maximising the log-likelihood:

\begin{equation} \label{eq:GPloglikelihood}
ln(L(\mathbf{x} | \boldsymbol{\theta})) = - \frac{p}{2} ln(2\pi) - \frac{1}{2} ln(|\boldsymbol{\Sigma}|) - \frac{1}{2} (\mathbf{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}).
\end{equation}

This is valid because the logarithm is a strictly monotonically increasing function, meaning the locations of turning points are invariant under the transformation. As the first term of the log-likelihood is independent of all the hyper-parameters, the problem of finding the MLE for \(\boldsymbol{\theta}\) can be mathematically described by:

\begin{equation}
\hat{\boldsymbol{\theta}} = \argmin_\theta{\{ln(|\boldsymbol{\Sigma}|) +(\mathbf{x}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}\}.
\end{equation}

The methods considered so far are exclusively applicable to single output models, therefore this report will begin my modelling the direct and indirect fire incidents independently. This means the optimization process will be carried out twice, and there will be a set of hyper-parameters for each of the data streams. \\

Optimization algorithms can compute the local minima given a function and a starting point, but it is difficult to determine when a global minima has been found. Given that the problem is in \(\mathds{R}^4\), a reasonable result should be obtained by using a grid search. This is where the optimiser is run repeatedly on using a grid of starting points which span a range of each hyper-parameter. The lowest scoring result of these optimisations is assumed to be the global minimum. \par

\section{Results}

Once the hyper-parameters have been chosen, the mean vector and covariance matrix of the GP for a denser set of times can then be generated. This process, known as inference, allows predictions to be generated for times which do not appear in the training data set. Forecasts can be made for times outside of the SIGACTS data period, but also for days inside the period but not in the training data. These predictions can then be directly compared to the actual number of incidents on those days, in order to evaluate the success of the model. \\

%The resulting vectors should form continuos curves, representing the underlying function which the observations are believed to be sampled from. \par
The model resulting from a training data set of 100 sample points is shown in Figure \ref{fig:GPresults}. The continuous line represents the function mean and the shaded area either side shows \(\pm\)2 standard deviations, which means that ideally 95\% of the data should be inside this area. The top graphs plot the training data points, while the bottom ones superimpose the entire data set (the testing data) over the predictions. The hyper-parameters selected for this model are listed in Table \ref{GPhyperparameters}.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=8.5cm]{GPresultsd.png}
  	\caption{Direct Fire}
  	\label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  	\centering
  	\includegraphics[width=8.5cm]{GPresultsi.png}
  	\caption{Indirect Fire}
 	 \label{fig:sub2}
\end{subfigure}
\caption{Gaussian process models from 100 sampled data points}
\label{fig:GPresults}
\end{figure}

\singlespacing
\begin{table}[]
\centering
\caption{Maximum likelihood estimates for the GP hyper-parameters}
\label{GPhyperparameters}
\begin{tabular}{c|c|c|c|c|}
\cline{2-5}
\textbf{}                                    & \(\mu\) & \(h\) & \(l\) & \(\sigma^2\) \\ \hline
\multicolumn{1}{|c|}{\textbf{Direct Fire}}   & 14.01           & 9.56          & 102.12          & 28.01              \\ \hline
\multicolumn{1}{|c|}{\textbf{Indirect Fire}} & 6.64           & 3.47          & 87.42          & 10.66              \\ \hline
\end{tabular}
\end{table}
\doublespacing

While the majority of the data set is within the 2 standard deviation range, this is due to the undesirably large levels of uncertainty predicted at all points in the model. More alarmingly, a significant amount of probability mass is placed on negative values -- while the actual number of incidents in a day is constrained to be both positive and integer. \par 

As continuous outputs are fundamental to the GP framework, it appears this model alone is not suitable for the SIGACTS data. 
%As producing continuous predictions is a fundamental part of the GP framework, 

% Also something about the direct failing to capture some characteristics near the end of the period

\chapter{Log Cox Gaussian Processes}

\section{Poisson Point Processes}
The Poisson distribution is defined only over (discrete) natural numbers, and expresses the probability of a given number of events occurring in a fixed interval of time. Its probability density distribution is defined by:

\begin{equation}\label{eq:poisson}
p(x=k|\lambda) = \mathcal{P}_o (x;\lambda) = \frac{1}{k!} e^{-\lambda} \lambda^k
\end{equation}

where \(\lambda\) is the expected number of events in a time interval, and \(x\) is the actual number observed. \cite{Barber}

\begin{equation}\label{eq:poisson}
 p(x=k|\lambda) = \mathcal{P}_o (x;\lambda) = \frac{1}{k!} e^{-\lambda} \lambda^k
\end{equation}

A Poisson point process is a natural extension of this, used for modelling instances of point events, where the number of events in a given time interval is drawn from some underlying Poisson distribution. The process is defined by an intensity function \(\lambda(x)\) which defines the underlying Poisson distribution at input \(x\). If this function varies with time then the process is labelled non-homogenous.\cite{Gregory} 

The likelihood of a set of independent identically (Poisson) distributed observations \(\mathbf{x}=[x_1,...,x_n]^T\) is given by:

\begin{equation}
p(x_1,...,x_n|\lambda) = \prod_{i=1}^{n} p(x_i|\lambda).
\end{equation}

Poisson point processes would be a more natural model for the SIGACTs data, as they only put weight on positive integer numbers. 

%These are defined by a single parameter \(\lambda\), if this is allowed to vary with time the process is labelled non-homogenous.

\subsection{Cox Processes}
A Cox process is essentially a non-homogenous Poisson point process where the underlying intensity \(\lambda\) is itself a stochastic process. In a Log Cox Gaussian process (LGCP) the log-intensity of the point process is modelled as a GP. \cite{LGCP-moller}

% YO I GOT TO HERE
\begin{equation} \label{eq:LGCPsetup}
\mathbf{v} = log(\boldsymbol{\lambda}) = \mathcal{G}\mathcal{P} ( \mu(. ;\boldsymbol{\theta}) , \mathbf{K}(. , . ,\boldsymbol{\theta}))
\end{equation}

This retains the expressivity of GPs while constraining the predicted intensities to be positive. However, the intensity values act as variables which need to be marginalised out, meaning the optimisation problem becomes significantly harder. To reduce the complexity, one of the hyper-parameters can be eliminated by selecting the constant mean function:

\begin{equation} \label{eq:LGCPmean}
\mu (x) = \frac{1}{N} \sum_{i=1}^{N} log_e(y_i),
\end{equation}

with the caveat that we take \(log(0)=0\) to avoid infinite terms in the summation when an observation is zero. This sets the constant mean to the average log-observation, thus avoiding learning its value. \\

The underlying characteristics of the log-intensity \(\mathbf{v}\) should be similar to the GP regression case, so the same (squared exponential) form for covariance function can be used - although the optimum hyper-parameters are likely to be significantly different.

\section{Regression}

The regression problem is more complicated; as well as the mean and covariance hyper-parameters, the discrete intensity values \(\mathbf{v}\) also need to be found. The training data \(\mathbf{D} = \{y_1,y_2, ..., y_N\}\) is composed of the number of observed incidents for a sample set of days. Unlike GPs, LGCPs are not capable of producing predictions for any input vector, but only for inputs corresponding to the observations. The resulting values of \(\mathbf{v}\) will then form the predicted mean of the model. The problem of predicting log-intensities at unobserved times is discussed later in this report.\\

Bayes rule can be used to formulate the following posterior distribution for \(\mathbf{v}\):

\begin{equation} \label{eq:LCGPposterior}
p(\mathbf{v | D}) = \frac{\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta})p(\boldsymbol{\theta}) d\boldsymbol{\theta}}}{\int{\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta})p(\boldsymbol{\theta}) d\boldsymbol{\theta} d\mathbf{v}}}},
\end{equation}

which the optimal vector \(\mathbf{v}\) will maximise. This expression requires a prior distribution of the hyper-parameters \(p(\boldsymbol{\theta})\) which can be approximated as the delta function:

\begin{equation}
p(\boldsymbol{\theta}) = \delta(\boldsymbol{\theta} - \hat{\boldsymbol{\theta}})
\end{equation}

where \(\hat{\boldsymbol{\theta}}\) are the \textit{true} parameters. The sifting property of delta functions can be exploited in order to reduce the posterior expression to:

\begin{equation} \label{eq:LCGPposterior2}
p(\mathbf{v | D}) = \frac{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}) }{\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}) d\mathbf{v}}}.
\end{equation}

For a point process model, the probability distribution for each observation \(y_i\) is Poisson with intensity \(e^{v_i}\). Assuming that observations are independent, the likelihood \(p(\mathbf{D|v})\) is just the product of the Poisson distributions:

\begin{equation} \label{eq:LGCPlikelihood}
p(\mathbf{D|v}) = \prod_{i=1}^{m} \mathcal{P}_o (y_i, e^{v_i}).
\end{equation}

As \(\mathbf{v}\) is a GP, its distribution given a set of hyper-parameters is just the multivariate Gaussian likelihood:

\begin{equation}
p(\mathbf{v}|\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}) = \frac{1}{\sqrt{(2\pi)^{m} |\hat{\boldsymbol{\Sigma|}}}} e^{(- \frac{1}{2} \mathbf{(v}-\hat{\boldsymbol{\mu}})^{T}\hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{v}-\hat{\boldsymbol{\mu}}))}.
\end{equation}


The denominator of Equation \ref{eq:LCGPposterior} is intractable, so an approximation is needed in order to compute the full posterior distribution. This report focuses on Laplace's approximation, where the integrand \(P(\mathbf{x})\) is assumed to be Gaussian and maximised at \(\mathbf{x=x_0}\). Equations \ref{eq:laplaceaprrox} and \ref{eq:laplaceaprrox2} give the form of the approximation\cite{Mackay}. 

\singlespacing

\begin{equation} \label{eq:laplaceaprrox}
\int{P(\mathbf{x}) dx} \approx P(\mathbf{x_0}) \sqrt{\frac{2\pi}{c}}
\end{equation} 

\begin{equation} \label{eq:laplaceaprrox2}
c = - \frac{\partial^2}{\partial \mathbf{x}^2} ln P(\mathbf{x_0}) |_{\mathbf{x}=\mathbf{x_0}}
\end{equation}

\doublespacing 

When applied to the expression \(\int{\{p(\mathbf{D|v}) p(\mathbf{v}|\boldsymbol{\theta}=\hat{\boldsymbol{\theta}})\} d\mathbf{v}} \) the result is only evaluated at the maximum \(\hat{\mathbf{v}}\) and therefore independent of the values \(v_i\). Therefore the maxima of the posterior and the product \(p(\mathbf{D|v}) p(\mathbf{v}|\boldsymbol{\theta}=\hat{\boldsymbol{\theta}})\) are at the same location.

We can concatenate the GP hyper-parameters and the log-intensity values into a single vector \( \boldsymbol{\gamma} = [\boldsymbol{\theta}, \mathbf{v}]^{T}\) which we can optimise over. Exploiting the monotonically increasing property of the \(log_e\) function, the problem can be written as:

\begin{equation} \label{eq:GPfmin}
\hat{\boldsymbol{\gamma}} = \argmin_\gamma{\{ \sum_{i=1}^{N}e^{v_i} - \mathbf{v}^{T}\mathbf{y} + \frac{1}{2}ln(|\boldsymbol{\Sigma}|) + \frac{1}{2}(\mathbf{v}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{v}-\boldsymbol{\mu})\}}
\end{equation}

The full posterior distribution \(p(\mathbf{v|D},\boldsymbol{\theta})\) can be approximated by the multivariate normal distribution:

\begin{equation} \label{eq:LGCPapproxpost}
p(\mathbf{v|D},\boldsymbol{\theta}) \approx \mathcal{N} (\mathbf{v}; \hat{\mathbf{v}}, \boldsymbol{\mathcal{H}}^{-1})
\end{equation}

where \(\boldsymbol{\mathcal{H}}\) is the Hessian of the log distribution evaluated at \(\hat{\mathbf{v}}\). This is defined as:

\begin{equation}
\boldsymbol{\mathcal{H}} = -\nabla^2 log p(\mathbf{v|D},\boldsymbol{\theta}) |_{\mathbf{v}=\hat{\mathbf{v}}}.
\end{equation}

\subsection{Reducing Complexity}

The optimization is now taking place in \(\mathds{R}^{N+3}\), where N is the number of training points used. As before, in order to find the global minima, the local minimum from a range of start points will be computed and the lowest of these minima is selected.

The GP model start points were distributed in 4 dimensional space meaning, given some prior knowledge of likely size of each hyper-parameter (for example that the length scale should be between 0 and the length of the full dataset) it is possible to achieve reasonable coverage of this space. However, in N+3 dimensions all points become very far apart, meaning decent coverage would require a infeasible number of start points.

While little is known about the values the noise variance, length and output scales are likely to take, the intensity values should be relatively close to the observations. 

The predicted intensities occur at times corresponding to the observed data, meaning the optimal \(v_i\) should be close to the observed \(log_e(y_i)\). The dimensionality of the start point space is reduced back to 3 if the optimiser is always started with \(\mathbf{v_0} = log_e \mathbf{y}\) so that the start point can be described by \ref{eq:LGCPstartpoint}. 

\begin{equation} \label{eq:LGCPstartpoint}
\boldsymbol{\gamma_0} = \left( \begin{array}{cc}
h_0 \\
l_0 \\
\sigma^2_0 \\
log_e(\mathbf{y}) \end{array} \right) 
\end{equation}

Only the 3 hyper-parameters are varied between the different start points, however the optimization is still in \(\mathds{R}^{N+3}\) so each run of the optimiser is significantly more computationally expensive than for the GP model, meaning it may not be feasible to use the same number of start points.  \par

The number of optimization calculations can be reduced by using Latin hypercube sampling which, given a range for each of the parameters, randomly selects start points which are dissimilar. \cite{latin-hyper} This reduces the chance of multiple start points leading to the same local minimum, and more efficiently covers the start point space. \\

The tolerance used by an optimiser strongly influences the time-cost of the operation, this is the change in function value between iterations at which the algorithm will stop. Many of the start points selected by the latin hypercube will be obviously sub-optimal, meaning it is wasteful to perform accurate optimization at these points. \par

This problem can be avoided by performing an initial round of optimization with a large function tolerance, which acts to identify the rough location of the global minimum. The output from this can be used as the start point for a single more precise optimization, where the local minimum found can reasonably be assumed to be global. \\

The most expensive part of the optimization algorithm is the inversion of the covariance matrix at each iteration. For a training dataset of length N the covariance has size \(N \times N\) and inverting large matrices is by nature a computationally intensive process. However, the necessary positive definite property of covariance matrices can be exploited in order to simplify this process. Cholesky decomposition is the factorization of a positive-definite matrix, \(\boldsymbol{\Sigma}\), into the product of a lower triangular matrix, \(\mathbf{L}\), and its conjugate transpose so that

\begin{equation}
\boldsymbol{\Sigma} = \mathbf{L} \text{ }\mathbf{L^T}.
\end{equation}

The inverse of the covariance is then given by:
\begin{equation}
\boldsymbol{\Sigma}^{-1} = ( \mathbf{L} \text{ }\mathbf{L^T} )^{-1} = \mathbf{L^{-T}} \text{ }\mathbf{L}^{-1}.
\end{equation}

Although the inverse of  lower triangular matrix is also lower triangular, these are still not trivial to compute. \par

The inverse covariance is never required on its own, only applied to the innovation vector. This means instead of computing the inverse and multiplying to get \(\mathbf{x=A^{-1}b}\) we can try to directly solve the linear system \(\mathbf{Ax=b}\). For the GP regression case the goal is given by:

\begin{equation}
\mathbf{x} = \boldsymbol{\Sigma}^{-1} \mathbf{d} = \mathbf{L^{-T}}\mathbf{L}^{-1}  \mathbf{d}.
\end{equation}

This can be found by successive solving the linear systems

\singlespacing
\begin{equation}
\mathbf{L \text{ } x_0}=  \mathbf{d}
\end{equation}

and then:

\begin{equation}
\mathbf{L^T \text{ } x}=  \mathbf{x_0}.
\end{equation}
\doublespacing

For triangular matrices solving these is deceptively simple; this can be easily seen by observing Equation \ref{eq:chol}, which shows an example 4x4 lower triangular linear system. The first element \(x_1\) can be solved instantly as \(c_1/L_{1,1}\), the second row can then be used to find \(x_2\) trivially and so on until all elements have been found.

\begin{equation} \label{eq:chol}
\left( \begin{array}{cccc}
L_{1,1} & 0  & 0  & 0  \\
L_{2,1} & L_{2,2}  &  0  & 0  \\
L_{3,1} & L_{3,2} & L_{3,3}  &0  \\
L_{4,1} & L_{4,2} &  L_{4,3} & L_{4,4}   \end{array} \right) \left( \begin{array}{cccc}
x_1  \\
x_2  \\
x_3  \\
x_4  \end{array} \right) = \left( \begin{array}{cccc}
c_1  \\
c_2  \\
c_3  \\
c_4  \end{array} \right) 
\end{equation}

MATLAB contains inbuilt functions for both Cholesky decomposition and solving linear systems in the manner describe, making it easy to compute \(\boldsymbol{\Sigma}^{-1} \mathbf{d}\) in this way. This dramatically reduces the time complexity of the calculation and, given the number of times the operation is performed, speeds up the optimization significantly. \par


\subsection{Initial Results}

The initial predictions produced with 60 sampled training points are shown in figure \ref{fig:LGCPresults}. The smaller training set was necessitated in order to reduce computation time, but otherwise the format is the same as for the GP results. \\

All probability mass is assigned to positive numbers, and the predicted variances are much smaller - meaning the level of uncertainty has been reduced. A preliminary look at the testing plots suggests that the majority of the observed  data lies within the predicted \(\pm\) 2 standard deviation area. However, the training data shows a worrying number of points outside of this region - far more than the ideal 5\% which could suggests this model is not a good fit. 

\par
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=8.5cm]{LGCPresultsd.png}
  	\caption{Direct Fire}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  	\centering
  	\includegraphics[width=8.5cm]{LGCPresultsi.png}
  	\caption{Indirect Fire}
\end{subfigure}
\caption{Log Gaussian Cox process models from 60 sampled data points}
\label{fig:LGCPresults}
\end{figure}

The hyper-parameters learned are listed in Table \ref{LGCPhyperparameters}. Similar length scales have been chosen for the data streams, which is unsurprising as changes to direct fire rates are likely to be closely related to those in indirect fire rates. 

%%%%%

\singlespacing
\begin{table}[]
\centering
\caption{Estimates for the LGCP hyper-parameters}
\label{LGCPhyperparameters}
\begin{tabular}{c|c|c|c|}
\cline{2-4}
\textbf{}                                    & \(h\) & \(l\) & \(\sigma^2\) \\ \hline
\multicolumn{1}{|c|}{\textbf{Direct Fire}}   & 0.8068          & 63.78          & \(1.304\times 10^{-8}\)              \\ \hline
\multicolumn{1}{|c|}{\textbf{Indirect Fire}} & 0.4098          & 61.12          & \(1.5672\times 10^{-7}\)              \\ \hline
\end{tabular}
\end{table}
\doublespacing

\section{Including Correlation}

Given the probable correlation between quantities it seems counterintuitive to construct two independent models for the data streams; not only will the hyper-parameters likely be similar, but if the events are correlated then the training data for one stream can provide valuable information for modelling the other.\par

This section explores a single model, which takes as training data samples of both the direct and indirect fire rates and outputs predictions for both using the approach outlined by \textit{Osborne et. al, 2008} \cite{multi-outputGP}.

\subsection{Model Adaptation}

The LGCP can be easily adapted to fit a multi-input, multi-output model. The training data streams are concatenated into a single vector:

\begin{equation} \label{eq:LGCPCy}
\mathbf{y} = \left( \begin{array}{cc}
\mathbf{y^{direct}} \\
\mathbf{y^{indirect}} \end{array} \right)
\end{equation}

and the mean function can be modelled as the step change: 

\begin{equation} \label{eq:LGCPCmean}
\mu (x) = \begin{cases}  
\displaystyle \sum_{i=1}^{N} log_e(y_i^{direct}) & for \text{ }x\le N \\
\displaystyle \sum_{i=1}^{N}log_e(y_i^{indirect}) & for \text{ }x>N \\ \end{cases}
\end{equation}.

The vector \(\mathbf{v}\) will now have length \(2N\), with the first \(N\) entries representing predictions of direct incidents, and the second of indirect. \par

The covariance function now takes four inputs: the time inputs \(x_1, x_2\) and the labels \(l_1,l_2\) which denote which data series the input value belongs to. For two points to be highly correlated they need to be both close in distance and the same type of incident so it is appropriate to model \(k\) as the product: 

\begin{equation} \label{eq:LGCPCcov}
k(x_1,x_2,l_1,l_2) = k_{SE}(x_1,x_2) \text{ } K_L(l_1,l_2) 
\end{equation}

where \(k_{SE}\) is the squared exponential function with unity variance and \(K_L\) is a square matrix whose size is equal to the number of data streams -- in this case two. \par

The squared exponential function \(k_{SE}\) is still appropriate for modelling the time component, as we still want the covariance between two points to decrease with separation in time. Both the direct and indirect fire rates must have the same length scale in order for the covariance matrix to be positive semi-definite, but they may have different output scales and noise variances.

The matrix \(K_L\) can be decomposed into a vector \(\boldsymbol{\lambda}\) which contains the respective output scales of the data streams and a matrix \(\mathbf{S}\) which models the correlation between data streams:

\singlespacing
\begin{equation} \label{eq:KL-decomp}
\begin{aligned}
K_L =& \boldsymbol{\lambda}^T \mathbf{S} \boldsymbol{\lambda} \\ \\
\boldsymbol{\lambda} = \left( \begin{array}{cc}
\rho_d  \\
\rho_i \end{array} \right)\text{ } & \text{ }
\mathbf{S} = \left( \begin{array}{cc}
1 & \alpha  \\
\alpha  & 1 \end{array} \right)
\end{aligned}
\end{equation}
\doublespacing

The diagonal elements of \(\mathbf{S}\) need to be 1 as a data stream is assumed to be perfectly correlated with itself, and all entries should be \(\in [0,1]\). The complete covariance matrix is then given by:

\begin{equation} \label{eq:LGCPCcovariance}
\begin{aligned}
\boldsymbol{\Sigma} =&  \left( \begin{array}{cc}
\rho_d^2 \mathbf{K} + \sigma_i^2 \mathbf{I} & \alpha \rho_d \rho_i \mathbf{K}  \\
\alpha \rho_d \rho_i \mathbf{K} & \rho_i^2 \mathbf{K} + \sigma_d^2 \mathbf{I} \end{array} \right) \\ where\\
&\mathbf{K}(i,j) = exp\left( \frac{(x_i-x_j)^2}{l^2}\right)
\end{aligned}
\end{equation}

with \(\rho_d\) , \(\rho_i\) representing the respective output scales, and \(\sigma_d^2\) , \(\sigma_i^2\) the noise variances.

There are now 6 hyper-parameters to be found, which are summarised in Table \ref{LGCPChyperparameters}. For the novel correlation coefficient it may not be necessary to try multiple dissimilar start points, as empirical methods can provide a reasonable estimate of the most likely value. \\

\begin{table}[]
\centering
\caption{Hyperparameters of the multi-input LGCP model}
\label{LGCPChyperparameters}
\begin{tabular}{ll|c}
\multicolumn{2}{c|}{\textbf{Hyperparameter}} & \textbf{Constraint} \\ \hline
\(\rho_1\)           & Direct output scale             & \(\geq0\)            \\
\(\rho_1\)             & Indirect output scale           & \(\geq0\)            \\
\(l\)          & Length scale                    & \(\geq0\)            \\
\(\alpha\)             & Correlation                     &  \(\in\{-1,1\}\)                   \\
\(\sigma_d\)             & Direct noise variance           &    \(\geq0\)                 \\
\(\sigma_i\)           & Indirect noise variance         &     \(\geq0\)               
\end{tabular}
\end{table}

Multivariate normal distributions allow simple analysis of the correlation between two randomly distributed variables. If the joint distribution of the direct and indirect fire incidents can be expressed as the 2-D Gaussian:

\begin{equation} \label{eq:2dmvg}
\left[ \begin{array}{cc}
y_{direct} \\
y_{indirect} \end{array} \right]
\sim
\mathcal{N} \left( \left[ \begin{array}{cc}
\mu_x \\
\mu_y \end{array} \right], \left[ \begin{array}{cc}
\sigma_{x}^2 & \sigma_{xy} \\
\sigma_{xy} & \sigma_{y}^2  \end{array} \right] \right)
\end{equation}


then the correlation coefficient can be approximated as:

\begin{equation} \label{eq:2dcorrelation}
\alpha = \frac{\sigma_{xy}}{\sqrt{\sigma_{x}^2 \sigma_{y}^2}}.
\end{equation}

Maximum likelihood can be used to find the best fitting 2D covariance matrix across the entire data set, the result of which is shown in Figure \ref{fig:correlation}. This predicts a correlation of \(\alpha \approx 0.4755\). \\

\begin{figure}
\centering
\includegraphics[width=12cm]{direct_indirect_correlation.png}
\caption{Empirical correlation between the observed direct and indirect incidents}
\label{fig:correlation}
\end{figure}

% Some positive correlation obvious, also i haven't given the figure yet

One major limitation of this method is that the covariance assigns probability mass to negative numbers. A natural extension might be to fit the 2D covariance to the log-observations and then transform the ellipse with an exponential, the results of which are illustrated in Figure \ref{fig:lcorrelation}. Although the covariance is now constrained to be positive, the ellipse has become distorted to the extent that \ref{eq:2dcorrelation} will no longer apply, and the raw correlation between the log-direct and log-indirect fire incidents isn't helpful.

\begin{figure}
\centering
\includegraphics[width=12cm]{direct_indirect_correlation2.png}
\caption{Empirical correlation between the logarithms of observed direct and indirect incidents}
\label{fig:lcorrelation}
\end{figure}

While the Gaussian empirical correlation may not be strictly accurate, it is still reasonable to assume the true correlation is close to this value. Therefore, the latin hypercube need only be used for the other hyperparameters with the empirical value for correlation at every start point. This will reduce the number of start points necessary to ensure good coverage of the probability space, while still allowing the optimal value of correlation to be learned. \\

% Also conditioning errors 

The optimisation problem is now in \(\mathds{R}^{2N+6}\), meaning the size of the training data used needs to be kept relatively low to avoid excessive computation time. The size of the set is also limited by the necessary gap between observations. If training data points are too close together the corresponding rows of the covariance become very similar, and the matrix becomes poorly conditioned meaning the inverse cannot be found. \par

If the training set is too small, there is a risk the model will not capture all features of the data. To ensure this isn't happening regression can be performed on multiple independent training sets and if consistent models were predicted then the data set was large enough.


\subsection{Results}
The predictions produced for a training sample of 70 points are shown in Figure \ref{fig:LGCPCresults}. The blue line represents the predicted mean of the direct data and the green the indirect, similarly the shaded areas represent the respective \(\pm\) 2 standard deviation bounds. The top plot shows only the training data points, while the bottom superimposes the predictions over the full data sets. \par

\begin{figure}
\centering
\includegraphics[width=13cm]{LGCPCresults.png}
\caption{Predictions for a multi-output LGCP model of both data streams}
\label{fig:LGCPCresults}
\end{figure}

\begin{table}[]
\centering
\caption{Optimum hyperparameters for multi-input LGCP model}
\label{LGCPChyper}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\(\rho_1\)} & \multicolumn{1}{c|}{\(\rho_2\)} & \(l\)                      & \(\sigma_d^2\)            & \(\sigma_i^2\)            & \(\alpha\) \\ \hline
0.4170                            & 0.8825                           & \multicolumn{1}{c|}{41.643} & \(3.9100 \times 10^{-4}\) & \(3.3323 \times 10^{-4}\) & 0.5772     \\ \hline
\end{tabular}
\end{table}

This model appears to capture the behaviour of the data set much more successfully than the previous one, which didn't incorporate correlation between the sets. This may be largely due to the model for each data stream having effectively twice as many data training points. There are also time complexity advantages to this model, as the optimization process only has to happen once.

% Also, negative stuff?

\chapter{Changepoints}

So far this report has focused on finding a single set of hyper-parameters which can accurately describe the entire dataset. However, as the period spans four years it's likely that the system dynamics change at some point during the conflict.

Changepoints can be incorporated into regression models at the expense of added hyperparameters, and hence complexity.

\section{Sudden Change in Output Scale}

The simplest case is a step change in only one of the hyper parameters, as considered in \cite{changepoint-prediction}. As the SIGACTs data shows a sharp increase in direct fire incidents towards the end of the period, a sudden drastic change in output scale may be an appropriate model.

This can be achieved by using the altered covariance function:

\begin{equation} \label{eq:outputcpcov}
k(x_1,x_2,l_1,l_2) = a(x_1,x_2)\text{ } K_L(l_1,l_2) \text{ }k_{se}(x_1,x_2)
\end{equation}

where \(k_{se}\) is the squared exponential kernel with unity output scale and \(K_L\) is the correlation matrix from the previous model and \(a\) is defined by:

\singlespacing
\begin{equation} 
a(x_1,x_2) = \begin{cases}
1 & for \text{ } x_1, x_2 \leq x_c \\
c & for \text{ } x_1, x_2 \geq x_c \\ 
\sqrt{c} & otherwise \\ 
\end{cases}
\end{equation}
\doublespacing

so that this function increases the output scales of both data series by a factor of \(c\) after some point \(x_c\). As \(a\) is a purely injective function, the resulting covariance remains positive definite. \par

Two new hyperparameters have been introduced: the scale factor \(c\) and the changepoint location \(x_c\) which is time value at which the change occurs. As time has been discretised into days \(x_c\) must be integer, and will also represent the index of the binned data series at the point of change. \par
%%%% should i define model evidence somewhere?
The integer constraint of the changepoint location means it cannot be optimised in the same way as the other parameters, using a gradient search. One possible solution is to try every possible location at each evaluation of the model evidence however, given the number of optimization iterations necessary, this would lead to unacceptable computation times. \\

Another option is to iteratively optimize over the changepoint location, then the other parameters until the change in prediction becomes smaller than some predefined constant. This means for every start point from the latin hyper-cube the most likely changepoint will be found, and then used for the first round of optimization. 

For each start point we can compute the posterior \(p(x_c | D, \boldsymbol{\theta})\) where \(D\) is the training data set, \(\boldsymbol{\theta}\) is the vector of the other hyper-parameters which are the suboptimal values selected by the latin hypercube. As \(x_c\) is an index the marginal takes the form of a discrete sum, giving the posterior distribution:

\begin{equation} \label{eq:xcposterior}
p(x_c | D, \boldsymbol{\theta}) = \frac{p(D,\boldsymbol{\theta},x_c)} {\displaystyle \sum_{i=1}^{n} p(D,\boldsymbol{\theta},x_c = t_i)}
\end{equation}

where the joint distribution \(p(D,\boldsymbol{\theta},x_c)\) is proportional to the quantity maximised previously in equation \ref{eq:GPfmin}.

Figure \ref{fig:oscpvariation} shows this computed posterior at several stages in the optimisation process. Every 5 iterations this distribution is computed and the most likely value used for the changepoint location for the next round of optimisation. As the other parameters become more optimal the probability mass becomes more concentrated at a single location, which should be expected as this is the value around which the other variables are being optimised. This figure shows the result from the most favourable of the start points selected by the latin hypercube.

\begin{figure}
\centering
\includegraphics[width=14cm]{learnchangea.png}
\caption{Progression of the (improper) posterior of changepoint location during optimization}
\label{fig:oscpvariation}
\end{figure}

The sudden drop-offs in the later distributions occur when the later changepoints result in poorly conditioned covariance matrices. When the optimisation is complete the covariance is poorly conditioned at almost every changepoint location besides the chosen one, meaning it is hard to quantify the certainty with which a changepoint is chosen. It should also be noted that due to the difficulty computing the later prior values, these distributions are not correctly normalised. \\

% This paragraph needs re-writing
While there is a consistent peak at around 400 days this is only marginally more likely than it's surrounding values. Perhaps more alarmingly, as the optimisation progresses the peak doesn't become more defined. This suggests that this sudden changepoint model might not be a good fit for the data.

The resulting model is shown in Figure 5.2, notice a longer length scale and a higher correlation with respect to the previous model has been chosen. While the results still look reasonable the model evidence, or the likelihood of the observed data given a model, is lower than without the changepoint. This is possibly because the longer length scale is failing to capture all behaviour. 
%%% CAN"T WORK OUT HOW TO GET LABEL TO WORK PROPERLY

  \begin{figure}[!ht]
    \centering
    \includegraphics[width=11cm]{LGCP_OSCPresults.png}
    \qquad
    \doublespacing
    \begin{tabular}[b]{cc}
    \multicolumn{2}{c}{\textbf{Hyperparameters}}                                            \\ \hline
      \(\rho_1\)                    & 1.0886                \\                       
\(\rho_2\)                     & 2.5159                           \\            
\(c\)                               & 1.3279                                      \\ 
\(x_c\)                        & 374                                         \\ 
\(l\)                               & 101.14                                      \\  
\(\sigma_d^2\) & \(4.7582 \times 10^{-4}\) \\ 
\(\sigma_i^2\) & \(4.2255 \times 10^{-4}\) \\ 
\(\alpha\)                      & 0.781              \\                      
    \end{tabular}
    \captionlistentry[table]{A table beside a figure}
    \captionsetup{labelformat=andtable}
    \caption{Results for a model with direct fire output scale changepoint}
  \end{figure}

\section{Sudden Change in Correlation}

The previous model assumes a constant value of correlation between the two data streams across the changepoint. This may not be appropriate, inspection of the raw data in Figure \ref{fig:sigactsdata} appears to show increased disparity of the data streams in the later half of the time period. 

When the empirical correlation of the data streams are calculated separately for the first and second halves it is found that \(\rho_{1^{st} half} \approx 0.6077 \) and \(\rho_{2^{nd} half} \approx 0.4913\), confirming that the correlation appears to change. This difference is illustrated in Figure \ref{fig:halfcorrelation}, which plots the covariance ellipses for the two halves separately.  \par
 
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=8.5cm]{halfcorrelation1.png}
  	\caption{First Half}
  	\label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  	\centering
  	\includegraphics[width=8.5cm]{halfcorrelation2.png}
  	\caption{Second Half}
 	 \label{fig:sub2}
\end{subfigure}
\caption{Empirical correlation of the data in two halves}
\label{fig:halfcorrelation}
\end{figure}

A sudden change in correlation can not be treated in the same way as a change in output scale; a step change in the value used would result in a covariance matrix that wasn't positive definite. One way to get around this is to directly model the difference between the data streams, instead of the individual incident types. This means the hyper-parameters learned will describe features of the relationship between the direct and indirect fire rates, rather than the data streams themselves. \par 

In order to keep both quantities being modelled positive, the two modelled quantities should be the indirect fire incidents (which are lower than the direct at every point in the data set) and the difference between the two. This is equivalent to expressing the log intensities \(v_d\) and \(v_i\) as a linear combination of two independent functions \(f\) and \(g\) which are defined in \ref{eq:ccpfandg}.

\begin{equation} 
\left[ \begin{array}{cc}
v_{d}(x)  \\
v_{i}(x) \end{array} \right] = \left[ \begin{array}{cc}
1 & 1  \\
1  & 0 \end{array} \right] \left[ \begin{array}{cc}
f(x)  \\
g(x) \end{array} \right]
\end{equation}

\begin{equation} \label{eq:ccpfandg}
\left[ \begin{array}{cc}
f(x)  \\
g(x) \end{array} \right] \sim \mathcal{G}\mathcal{P} \left( \left[ \begin{array}{cc}
\mu_i  \\
\mu_d - \mu_i \end{array} \right], \left[ \begin{array}{cc}
K_f & 0  \\
0  & K_g \end{array} \right] \right)
\end{equation}

The covariance of the indirect log-intensities \(K_f\) can take the same form used earlier in this report, where LGCPs were used to model the data streams independently, so that the covariance function is given by \ref{eq:kf} containing two hyper-parameters: the length and output scales. 

\begin{equation} \label{eq:kf}
k_f(x_1,x_2) = \rho_1 \text{ }exp\{\frac{-(x_1-x_2)^2}{2 l_1^2}\}
\end{equation}

Modelling the difference function \(g\) directly allows much more complex analysis of the change in relationship between the two data streams, which appears to change throughout the period. Therefore it is the covariance \(K_g\) which should capture the changepoint. As there is an apparent increase in the variation between the data stream the change could be modelled by an increase in the output scale. The data is still a time-series so a squared exponential covariance is still appropriate therefore \ref{eq:kg} is used, where \(a\) has the same form as in \ref{eq:outputcpcov2}.

\begin{equation} \label{eq:kg}
k_g(x_1,x_2) = \rho_2 \text{ }exp\{\frac{(x_1-x_2)^2}{2 l_2^2}\} \text{ } a(x_1,x_2)
\end{equation}


As the training data consists of observations of direct and indirect incidents, the mean vector and covariance matrix need to be transformed into this form. Using standard multivariate Gaussian identities, for a transformation of the form \(\mathbf{y}= \mathbf{M} \mathbf{x}\) these arrays are given by \(\boldsymbol{\mu_y}= \mathbf{M}\text{ }\boldsymbol{\mu_x}\) and \(\mathbf{cov_y}= \text{ }\mathbf{M}\text{ } \mathbf{cov_x} \text{ } \mathbf{M}^{T}\).par

The observations can also be assumed to be noise corrupted. In previous models maximum likelihood estimation has selected very similar values for the noise variance of the individual data streams. Therefore, in the interests of reducing the number of hyper-parameters, the noise on all observations can be modelled as the same. This means the log-intensities of the data streams can be modelled by the Gaussian process defined in \ref{eq:fandggp}. These equations are in the familiar LGCP form, so we can optimise over the hyperparameters in the same fashion as the previous model. 

\begin{equation} \label{eq:fandggp}
\left[ \begin{array}{cc}
v_d(x)  \\
v_i(x) \end{array} \right] \sim \mathcal{G}\mathcal{P} \left( \left[ \begin{array}{cc}
\mu_d  \\
\mu_i \end{array} \right], \left[ \begin{array}{cc}
K_f + K_g & K_f  \\
K_f  & K_f  \end{array} \right] + \sigma^2 \mathbf{I} \right)
\end{equation}

The evolution of the likelihood variation of changepoint is shown in Figure \ref{fig:cccpvariation}. This time a much more convincing peak at around 800 days is chosen, although conditioning errors still prevent the certainty of the final location being quantified. With each iteration less probability mass is being assigned to other changepoint values, which should be expected as the other variables are being optimised around the chosen value. This more reassuring plot suggests that a change in correlation may fit the data better than a change in output scale. \par

% Also, changepoint in much more expected place

\begin{figure}
\centering
\includegraphics[width=16cm]{learnchange.png}
\caption{Variation of likelihood with Changepoint location}
\label{fig:cccpvariation}
\end{figure}

The results of this model are shown in Figure 5.5, appearing to fit the data much better than the output scale changepoint model did. It is interesting to note that the length scale chosen for the difference function is more than twice as long as the one chosen for indirect fire.

%more about results

%% LABELLING PROBLEM WARNING NUMERO 2
  \begin{figure}[!ht]
    \centering
    \includegraphics[width=11cm]{LGCPCCPresults.png}
    \qquad
    \doublespacing
    \begin{tabular}[b]{cc}
    \multicolumn{2}{c}{\textbf{Hyperparameters}}                                            \\ \hline
      \(\rho_1\)                    & 0.3492                \\                       
\(\rho_2\)                     & 0.2285                           \\            
\(c\)                               & 2.8076                                      \\ 
\(x_c\)                        & 874                                         \\ 
\(l_1\)                               & 37.6009                                      \\  
\(l_2\) & 76.8452 \\ 
\(\sigma^2\) & \(7.1044 \times 10^{-4}\) \\ 
& \\
    \end{tabular}
    \captionlistentry[table]{A table beside a figure}
    \captionsetup{labelformat=andtable}
    \caption{Results for a model with correlation changepoint}
  \end{figure}

One notable difference between this model and its predecessors is its performance at the end of the data period. This is reassuring, given that the motivation for exploring the correlation changepoint was to model the apparent separation of the data streams in later years. The previous models all parameterised a constant correlation parameter which constrained the difference between the data stream predictions, and removing this has allowed more detail to be captured.

This model also saw a reduction in time complexity, probably due to the smaller number of hyperparameters. Unfortunately the training data set is still limited to the same size by conditioning errors, so this reduction in running time doesn't allow an increase in training data points.

\subsection{Incorporating Change in Mean}

Given that this model assumes a change in the direct fire data stream, a constant mean function no longer seems appropriate; clearly the values after the changepoint are typically higher than those before it. Currently the prior mean is set to the mean of the observed data, it should be noted that this may not be the most likely estimate for the constant mean function but this approach has been adopted to simplify the computation. \par

If the previous model was used for extrapolation, then the predicted values would quickly revert to the mean function. This means that if points from the end of the period were not included in the training data set, and these dates were predicted for the model would perform very poorly. \par

In the interests of not adding hyper-parameters a simple solution would be to model the mean function as a step change, as described in equation \ref{eq:cpmeaneq} where \(x_c\) is the time of the change point and \(n\) is its index in the \(N\)-length training data set.

\begin{equation} \label{eq:cpmeaneq}
\mu_d = \begin{cases}
\frac{1}{n} \displaystyle \sum_{i=1}^{n} log_e(y_i) & for \text{ } x \leq x_c \\
\frac{1}{N-n} \displaystyle \sum_{i=1}^{N-n} log_e(y_i) & for \text{ } x > x_c \\ 
\end{cases}
\end{equation}

Effectively the mean of the direct fire incidents before and after the changepoint are now being modelled as separate functions. This is not intuitive, but may result in a better fitting model. \par

The predictions produced by this updated model are shown in Figure 5.6 and the variation in likelihood with changepoint location is shown in Figure \ref{fig:cmcpvariation}. 
% THIRD WARNING ABOUT LABELLING, yawn....
  \begin{figure}[!ht]
    \centering
    \includegraphics[width=11cm]{LGCP_CMCP.png}
    \qquad
    \doublespacing
    \begin{tabular}[b]{cc}
    \multicolumn{2}{c}{\textbf{Hyperparameters}}                                            \\ \hline
      \(\rho_1\)                    & 1.1549                \\                       
\(\rho_2\)                     & 0.4891                           \\            
\(c\)                               & 4.8005                                      \\ 
\(x_c\)                        & 437.0000                                         \\ 
\(l_1\)                               & 33.2112                                      \\  
\(l_2\) & 87.4925 \\ 
\(\sigma^2\) & \(9.9799 \times 10^{-5}\) \\ 
& \\
    \end{tabular}
    \captionlistentry[table]{A table beside a figure}
    \captionsetup{labelformat=andtable}
    \caption{Results for a model with correlation + mean changepoint}
  \end{figure}
  
\begin{figure}
\centering
\includegraphics[width=14cm]{learnchangecm.png}
\caption{Variation of likelihood with Changepoint location}
\label{fig:cmcpvariation}
\end{figure}

The most significant difference between the models is the position of the changepoint, which the latter model predicts over a year earlier. This is especially interesting given that both models predict their respective changepoints with relative certainty and consistently. \par

This could possibly indicate the existence of more than one changepoint, or that the sudden change model is ill-fitted to the evolution of the system. 

% something else?

\chapter{Generating Predictions}

In order to properly compare the results of the models explored in this report predictions need to be generated using a common set of test data inputs \(\mathbf{x_*}\), for which observations \(\mathbf{y}_*\) are available. If each model \(i\) is used to generate \(f_i(\mathbf{x_*})\) the relative success of the algorithms in predicting \(\mathbf{y}_*\) can be assessed. \par

In this case the test inputs will be a vector of day numbers, and the models will try to predict the aggregate number of direct and indirect incidents that occurred on each of those days. The testing data should be independent of the model, so the days chosen can not have been used in the training data set of any of the models. Given the large amount of data available, this constraint is not restrictive. \par

For the GP models evaluating \(f(\mathbf{x_*})\) is trivial; full probability density functions are already being generated for every day within the period. However, for the LGCP models mean and variance values are currently only being calculated for days corresponding to the training data. \par

One possible work around is to linearly interpolate predictions from the two nearest available data points. Given the generally smooth varying nature of models this should return reasonable results, however this will not be accurate enough to distinguish between similar models. Therefore, given a LGCP model with chosen hyper-parameters predictions need to be generated for time steps not observed.
%For fair comparisons, full predictions need to be generated for the test input times using the LGCP models.

\section{LGCP Inference}

Assume a LGCP model with chosen mean and covariance functions \(\hat{\mu}\) and \(\hat{k}\) which has been generated using training data \(\mathbf{D} = (x_i,\mathbf{y_i})\) for  \(i = 1,...,m\), where \(x_i\) represents the day of the \(i\)th observation and \(\mathbf{y_i}\) contains the number of direct and indirect fire incidents on that day. Now given a new set of inputs \(x_j\) where \(j = 1,...,n\) we want to predict the number of both type of incidents on the \(j\)th day, in other word we want to generate:

\begin{equation}
p(\mathbf{v | D}) = \frac{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta}) }{\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta}) d\mathbf{v}}}\end{equation}

for a \(m+n\) length \(\mathbf{v}\) corresponding to both the observed and the test time inputs. For clarity sake we denote the new input vector \(\mathbf{x_*} = [x_i,....x_m,...,x_{m+n}]\) where the first \(m\) entries contain the times of the observed data, and the last \(n\) entries correspond to the test times. \par

The likelihood term \(p(\mathbf{D|v})\) is independent of the new test times so still has the form which was used for model optimization, repeated for convenience in \ref{eq:LCGPinflik}. The prior \(p(\mathbf{v}|\boldsymbol{\theta})\) will contain terms of \(\mathbf{v}\) relating to both the training and test data inputs, and can be expressed in terms of \(\hat{\boldsymbol{\mu}}\) and \(\hat{\boldsymbol{\Sigma}}\), the mean vector and covariance matrix generated for the input vector \(\mathbf{x_*}\) using the chosen hyper-parameters.

\singlespacing
\begin{equation} \label{eq:LCGPinflik}
p(\mathbf{D|v}) = \prod_{i=1}^{m} \mathcal{P}_o (y_i, e^{v_i})
\end{equation}

\begin{equation}
p(\mathbf{v}|\boldsymbol{\theta}) = \frac{1}{\sqrt{(2\pi)^{m+n} |\hat{\boldsymbol{\Sigma|}}}} exp(- \frac{1}{2} \mathbf{(v}-\hat{\boldsymbol{\mu}})^{T}\hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{v}-\hat{\boldsymbol{\mu}}))
\end{equation}
\doublespacing

The marginal integral is intractable, so an approximation method is necessary in order to compute the posterior. Monte-Carlo methods can be used, where the integrand is approximated by sampling one distribution and inputting the results into the other. The law of large numbers implies that if this is done enough times, and the results averaged then the expectation should be representative. The approximate marginal is given by:

\begin{equation}
\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta}) d\mathbf{v}} \approx \frac{1}{N} \displaystyle \sum_{k=1}^{N} p(\mathbf{D|v=v_k})
\end{equation}

where \(\mathbf{v_k}\) is a draw from the distribution \(\mathcal{N}(\mathbf{v_k};\hat{\boldsymbol{\mu}},\hat{\boldsymbol{\Sigma}})\), and \(N\) is some arbitrary large number. Due to the standard form of the prior distribution, sampling can be easily achieved using inbuilt functions in Matlab. The prediction problem then becomes 

\begin{equation}
\hat{\mathbf{v}} = \argmin_\mathbf{v}{\{  \mathcal{N}(\mathbf{v};\boldsymbol{\mu, \Sigma}) \text{  }\text{  } \displaystyle \prod_{i=1}^{m} \mathcal{P}_o (y_i, e^{v_i}) \text{  }\text{  } Zp^{-1} } \}
\end{equation}

where \(Z_p\) is the approximate marginal obtained using Monte Carlo integration. Note while there are only \(m\) poisson terms in this expression, the multivariate Gaussian is in \(\mathcal{R}^{n+m}\) as it features both training and testing inputs. \par

It is simpler to maximise the log of this expression, as several terms may be neglected. Due to the fixed covariance and mean functions this optimization operation is significantly easier than those attempted earlier in this report. \par

Figure \ref{fig:inference} shows an example result of this prediction procedure. This example was for one of the later LGCP models which incorporates a change point in covariance. The predicted mean and \(\pm 2 \sigma\) are plotted in blue for the direct and green for indirect, and the training data is plotted with x's. The predictions at test times are show in red for both data streams, with o's marking the predicted mean and error bars the \(\pm 2 \sigma\) limits. For graphical clarity only four test data points have been used in this example, but similar results can be obtained for much larger data sets.

\begin{figure}[h!]
\centering
\includegraphics[width=17cm]{prediction.png}
\caption{Prediction for 4 test inputs using an LGCP model}
\label{fig:inference}
\end{figure}

Although this graph only give the predicted values with some confidence region, it is possible to generate the full probability distribution for each predicted input. Figure \ref{fig:inference2} plots in red the full posterior for the predicted direct and indirect fire incident intensities for a single test day. It should be noted that these are (continuous) distributions of the poisson intensities, not the (discrete) distributions over the number of observed incidents. 

\begin{figure}[h!]
\centering
\includegraphics[width=17cm]{prediction_dist.png}
\caption{Full posterior predictions for one test input}
\label{fig:inference2}
\end{figure}
% PAGES SORT IT OUT MOTHERFUCKER
\newpage

\chapter{Testing}

It's useful to have a formal method of evaluating and comparing the success of models. Given a set of predictions of mean and variance and a testing data set we want to assign a score reflecting the success of the model. It is important that the testing data set is independent of the training set in order to fairly analyse the model. \par

Given testing data composed of observations \(\mathbf{z_i}\) containing the number of direct and indirect fire rates on the \(i\)th day, the first task is to identify the model predictions at that time step. All models presented in this report model the posterior distribution of the number of incidents with a gaussian, so these predictions will just be a mean and variance for each data stream at every testing time step. Once the predicted distributions for each of the models have been generated results can be directly prepared.

A summary of the models produced in this report is given in Table \ref{tab:model-description}.

\begin{table}[]
\caption{Description of the models explored in this report}
\label{tab:model-description}
\centering
\begin{tabular}{c|p{14cm}}
\textbf{Model} & \multicolumn{1}{c}{\textbf{Description}}                                                                                                                                                          \\ \hline
& \\
\bf{1}              & Two independent GPs, one for each data stream                                                                                                                                 \\
& \\
\bf{2}              & Two independent LGCPs, one for each data stream                                                                                                                               \\
& \\
\bf{3}              & One multi-output LGCP which incorporates correlation between the two data streams                                                                                             \\
& \\
\bf{4}              & (3) but with a changepoint in the output scale of the direct fire incidents                                                                                                   \\
& \\
\bf{5}              & A multi-output LGCP which models the indirect fire and the difference function between the two data stream, with a changepoint in the output scale of the difference function \\
& \\
\bf{6}              & (5) but with a changepoint in the mean function at the changepoint of the difference output scale                                                                            
\end{tabular}
\end{table}

\section{Data with known Hyperparameters}

While the form of the model is paramount, it is the process of tuning the hyperparameters which fits the predictions to the data. Therefore, before making conclusions on the success of a model, we need to be sure the best fitting set of hyperparameters have been found.

One way of doing this is to generate a test data set from a model with chosen hyperparameters. For example, in the LGCP case, a vector of hyperparameters \(\boldsymbol{\theta}\) would be chosen such that the log-intensities were believed to be distributed as in \ref{eq:mvnrand}. For some vector of inputs \(\mathbf{x}\) values for \(\mathbf{v}\) would then be generated by a draw from the multivariate Gaussian distribution. These values would then be used to set up the Poisson distribution in \ref{eq:porand} which can be sampled to obtain some test observations \(\mathbf{y}\).

\singlespacing
\begin{equation} \label{eq:mvnrand}
\mathbf{v(x)} \sim \mathcal{N}\{\mu(\mathbf{x}), \mathbf{K(x},\boldsymbol{\theta})\}
\end{equation}

\begin{equation} \label{eq:porand}
\mathbf{y} \sim \mathcal{P}_o (e^{\mathbf{v}})
\end{equation}
\doublespacing

\begin{figure}
\centering
\includegraphics[width=16cm]{testmodel.png}
\caption{Four draws from the same test model}
\label{fig:testmodel}
\end{figure}

This is not a method of comparing models, but merely ensuring the full potential of a models structure is being reached. It should be noted however, that this test is flawed, as there may be two sets of hyperparameters which describe the data equally well - for example there is a trade of between length scales and observational noise. This means that while a model could be deemed to have 'failed' this test, it could still be accurately modelling the data. \par

Table \ref{tab:inmodelcompare} shows an example comparison of the true hyperparameters with those learned for model 4. In this case while all values learned are of the correct order, they are not found with exceptional accuracy. However, as discussed previously, it may be that the chosen hyperparameters describe the data equally well as the true values.

\begin{table}[]
\centering
\caption{An example of a comparison produced using data from a known model}
\label{tab:inmodelcompare}
\begin{tabular}{c|cccccccc}
                  & \(\rho_1\)      & \(\rho_2\)      & \(c\)         & \(x_c\)        & \(l\)         & \(\sigma_d^2\)    & \(\sigma_i^2\)    & \(\alpha\)     \\ \hline
\textbf{True}  &  0.6 &  0.8         &  1.2         &    800       &       100    &    \(1.0 \times 10^{-3} \)      &    \(2.0 \times 10^{-3} \)              &    0.5       \\
\textbf{Estimate} &      0.4913     &    0.8460       &    1.6596       &    724       &     81.5916      &    \(1.4436 \times 10^{-4} \)       &     \(4.0295 \times 10^{-4} \)      &      0.6124     \\ \hline
\end{tabular}
\end{table}

\section{Evaluation Methods}
The mean square error is a crude but effective evaluation tool, which measures the square distance between the training data observations and the predicted mean at that time step and sums over all testing data. The calculation is given in \ref{eq:mse}, where an N-length set of testing data has been used containing observations \(y_i\) corresponding to known inputs \(x_i\). This would produce an index of the success of the model for a single data stream, meaning scores for both the direct and indirect incident data would be obtained in each case. 

\begin{equation} \label{eq:mse}
mse = \frac{1}{N} \sum_{i=1}^{N} (y_i - f(x_i))^2
\end{equation}

While this measure gives some idea of the accuracy of the predicted mean, no account is taken of the model's predicted variance. Clearly it's preferable that the model perform badly in an area where it has declared a large uncertainty than in places where the predicted variance is low. \par

A better solution is to find the model evidence, or the probability of the testing data occurring given the model. This strictly requires the full posterior distribution for each of the generated models, which are intractable in the LGCP case. For simplicity we can assume that each observation \(y_i\) is independently Gaussian distributed, as in \ref{eq:glikelihoodobs}, with mean and variance equal to those obtained from the predictions. The log-evidence is then the sum of the log-likelihoods at all of these test observations, as given in \ref{eq:loglikelihoodobs}.

\singlespacing
\begin{equation} \label{eq:glikelihoodobs}
p(y_i) \approx \mathcal{N}(y_i;f(x_i),\sigma_i)
\end{equation}

\begin{equation} \label{eq:loglikelihoodobs}
log_e |p(D|\mathcal{M})| =  \displaystyle \sum_{i=1}^{N} log_e |p(y_i = k)|
\end{equation}
\doublespacing

The final model evidence is simply \ref{eq:loglikelihoodobs} exponentiated, however for large testing data sets this quantity may become too small to compute. One simple work around to this problem is to instead compute the average log-likelihood exponentiated, such that the score assigned to the model is given in \ref{eq:modelevidencescore}. Unlike the complete model evidence this result has no practical application, but will still award higher values to distributions which fit better. Furthermore, if the size of the testing data is held constant at \(N\) samples, this result should just be the model evidence raised to the power \(\frac{1}{N}\). 

\begin{equation} \label{eq:modelevidencescore}
score = exp \{ \frac{1}{N} \displaystyle \sum_{i=1}^{N}  log_e |p(y_i = k)|  \}
\end{equation}

Both of these scoring criteria can be used to evaluate all of the models explored in report, and each will produce a score for both the direct and indirect fire incident rates.

\section{Comparison}

Given a testing data set of 500 points, the results of both evaluation methods for each model described in Table \ref{tab:model-description} are listed in Table \ref{model-comparison}. \par

The units of the mean square error test are incidents\(^2\), so lower numbers are preferable as they signify that the testing points are closer to the predicted mean. Contrastingly the model evidence score represents the probability of a model being correct, so higher numbers are preferable.

\begin{table}[]
\centering
\caption{Formal evaluation of the different models}
\label{model-comparison}
\begin{tabular}{ccccc}
\multicolumn{1}{l}{}                                       & \multicolumn{4}{c}{{\ul \textbf{Test}}}                                                                                                                                           \\
\multicolumn{1}{c|}{\multirow{2}{*}{{ \textbf{Model}}, \(\mathcal{M}\)}} & \multicolumn{2}{c|}{\textbf{Mean Square Error}}                                         & \multicolumn{2}{c}{\textbf{Model Evidence}}                                                         \\
\multicolumn{1}{c|}{}                                      & \multicolumn{1}{c|}{\textit{Direct Fire}} & \multicolumn{1}{c|}{\textit{Indirect Fire}} & \multicolumn{1}{c|}{\textit{Direct Fire}} & \multicolumn{1}{c}{\textit{Indirect Fire}} \\ \hline
\multicolumn{1}{c|}{\textbf{1}}                    & \multicolumn{1}{c|}{29.1917}              & \multicolumn{1}{c|}{11.5186}                & \multicolumn{1}{c|}{0.0424}               & \multicolumn{1}{c}{0.0691}                 \\ \hline
\multicolumn{1}{c|}{\textbf{2}}            & \multicolumn{1}{c|}{32.9365}              & \multicolumn{1}{c|}{15.4097}                & \multicolumn{1}{c|}{0.0476}               & \multicolumn{1}{c}{0.0134}                 \\ \hline
\multicolumn{1}{c|}{\textbf{3}}    & \multicolumn{1}{c|}{25.5107}              & \multicolumn{1}{c|}{15.1999}                & \multicolumn{1}{c|}{0.0423}               & \multicolumn{1}{c}{0.1093}                 \\ \hline
\multicolumn{1}{c|}{\textbf{4}}            & \multicolumn{1}{c|}{26.6440}              & \multicolumn{1}{c|}{15.3058}                & \multicolumn{1}{c|}{0.0284}               & \multicolumn{1}{c}{0.0240}                 \\ \hline
\multicolumn{1}{c|}{\textbf{5}}    & \multicolumn{1}{c|}{25.2765}              & \multicolumn{1}{c|}{13.2748}                & \multicolumn{1}{c|}{0.0854}               & \multicolumn{1}{c}{0.0482}                 \\ \hline
\multicolumn{1}{c|}{\textbf{6}}    & \multicolumn{1}{c|}{26.2384}              & \multicolumn{1}{c|}{13.5779}                & \multicolumn{1}{c|}{0.1531}               & \multicolumn{1}{c}{0.0869}                 \\ \hline
\end{tabular}
\end{table}

Examining the numerical results immediately shows that there is no one model that scores best in all tests. \par

In general probabilistic modelling involves a compromise between model complexity and expressivity; If a model is too simple then it will not capture all features of the function it is trying to model, while an overly complex model (one with too many parameters) will spread its probability mass over too large an area, meaning it will not predict anything will significant certainty. \par

It is clear that \(\mathcal{M}=3\), the initial multi-output LGCP performs uniformly better than both \(\mathcal{M}=1,2\) which are the GP and single output LGCP models. This implies that moving to LGCPs and modelling both streams at once were both beneficial forms of added complexity. \par

Contrastingly \(\mathcal{M}=4\), where an output scale changepoint was introduced performs unanimously worse than its predecessor. This may seem counter-intuitive, as with a scale factor of 1 the models become the same, but these added redundancies increase the model uncertainty. \par

Models 5 and 6 took a different approach to modelling in order to better analyse the relationship between the two data streams, and capture a changepoint in their correlation. Unlike in \(\mathcal{M}=4\) the introduction of the changepoint has significantly improved the model evidence, suggesting that this is an appropriate model for the data, and that the added complexity has allowed the model to capture behaviour that it did not before. \par

The difference between models 5 and 6 is in the mean function; \(\mathcal{M}=5\) uses a constant mean for each data series, while \(\mathcal{M}=6\) models a step change in the direct fire mean at the position of the changepoint. This adaptation drastically improved (nearly doubled) the model evidence for both data streams, although interestingly the mean square error increased slightly. 

While it may appear obvious to select model 6 over 5, questions arise over the feasibility of the model; It is very hard to imagine an observable quantity undergoing a step change, especially one as drastic as that inferred by \(\mathcal{M}=6\). This \textit{could} be an example of a model which was designed to perform well in a test rather than meaningfully represent the data, which should be avoided. This increased score appears to indicate that the constant mean function is not optimal, if further work were to be carried out, a wider range of mean functions might be investigated. Smoother varying operators such as a sigmoid or linear function could be of use, although these come at the expense of added hyper-parameters (and therefore complexity). \par

Although it is the only thing quantified here, the quality of fit is not the only criteria for a good model. For example, time complexity and consistency of predictions are also important. \par 

By far the fastest model was \(\mathcal{M}=1\), the GP model, however this benefit is overshadowed by its drastically inferior fit. More notably, \(\mathcal{M}=5,6\) which directly modelled the difference between the data streams took significantly less time than \(\mathcal{M}=3,4\) which modelled the individual data streams. 


\chapter{Conclusion}

This report explored a subset of the SIGACTS data from the Afghanistan War. This dataset is the most detailed of its kind to be published, and modelling it may offer insight into the progression of the war. \par

The first models explored were Gaussian Processes, and although these offered complete tractability they proved ill suited to modelling deaths which, as discrete variables, are constrained to natural numbers. \par

Greater success was found using Log-Cox Gaussian Processes, which constrain predictions to be positive by placing the Gaussian Process prior on the log of the intensity function. These models were much more computationally expensive and therefore a very limited amount of training data could be used to generate the models. \par

By creating one model which made predictions both datasets the correlation between the different type of incidents was quantified and analysed. This improved the quality of the predictions not least because it effectively doubled the training data set. \par

The use of changepoint models was investigated in the final parts of this reports. At first limited success was found using the 'single-jump sudden change' models covered extensively in previous research, this was largely because none of the hyperparameters describing the model directly described the dynamic which was changing. \par

This motivated a redesign of the algorithm to directly model the difference between the data streams. In effect, this allowed a changepoint in correlation to be modelled using the single-jump model. These models performed better in terms of both fit and computation effort, however subtly different models learned dissimilar positions for the correlation changepoint. This suggests that a single-jump model may not be optimal for this dataset. \par

Overall, it is clear that the LGCP models were well-suited to this type of data and that the second approach to modelling was computationally superior. The main limitation of these models was the size of training data set that could be used, particularly given the vast amount of data available. This is due to both computational complexity and conditioning errors arising in the inverse covariance matrix when training inputs are too similar. In order to achieve vastly more accurate models one or both of these issues would be need to be resolved. \par

In terms of the existence of a changepoint, the results are not as clear. Better fitting models were achieved using changepoint models, and manual inspection of the dataset does appear to show a change in the system dynamics. However these models made unconfident predictions of changepoint location and the model evidence was, in most cases, only marginally improved. A more complex changepoint model may be able to better capture the change in dynamics, but would likely significantly worsen the computational cost. 

\singlespacing 

\newpage

\bibliographystyle{plain}
\bibliography{report}

\end{document}
