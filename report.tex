\documentclass[a4paper,11pt]{report}

%\usepackage[margin=1in]{geometry}

% This sets the margins. They can be reduced to 20mm if i need the room later!
\usepackage{geometry}
 \geometry{
 a4paper,
 left=20mm,
 top=30mm,
 }

\author{Constance Crozier\\Department of Engineering Science, University of Oxford}
\title{Bayesian Non-Parametrics for Data from the War in Afghanistan}
\date{May 2016}

% In order to import matlab figures
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

% This sets the font to arial
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}

% This changes the heading distances at the start of chapters
\usepackage{titlesec}
\titleformat{\chapter}[display]   
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{20pt}{\Huge}   
\titlespacing*{\chapter}{0pt}{10pt}{15pt}

% This stops a new page from being created
\usepackage{etoolbox}
\makeatletter
\patchcmd{\chapter}{\clearpage}{}{}{}
\makeatother


% This changes the form of the chapter title
\usepackage[T1]{fontenc}
\usepackage{titlesec, blindtext}
\newcommand{\hsp}{\hspace{20pt}}
\titleformat{\chapter}[hang]{\LARGE\bfseries}{\thechapter\hsp}{0pt}{\LARGE\bfseries}
\titleformat{\section}[hang]{\large\bfseries}{\thesection\hsp}{0pt}{\large\bfseries}
% For table and figure
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}



% This lets us use double spacing
\usepackage{setspace}
\doublespacing

% This lets me use the normal operator, plus a whole bunch of other fun maths symbols 
\usepackage{amssymb}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{dsfont}
\usepackage{mathtools}

\usepackage{cite}

% This is for underlining in tables
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}

\begin{document}
\maketitle

\begin{abstract}
hi
\end{abstract}

\singlespacing
\pagestyle{plain}
\tableofcontents
\doublespacing

\pagebreak

\chapter{Introduction}
\section{Background}

In October 2001, in reaction to the 9/11 attacks, the United States invaded Afghanistan. Although only initially supported by only close allies, including the United Kingdom, they were joined by NATO in 2003. \par

Operation Enduring Freedom lasted until the 31st December 2014, and was estimated to have lead to the deaths of at least 55,000 insurgents and 21,200 civilians.\cite{bodycount}

On the 25th July 2010 WikiLeaks released over 75,000 secret US military reports detailing events in Afghanistan from 2004 to 2010. \cite{wikileaks} The publication was intended to provide the public with a more comprehensive understanding of the reality of the war in Afghanistan.

\section{SIGACTS Data}
This report examines the SIGACTS data, which forms the base of the 'Afghan War Diary'. This comprises all the significant activities logged by coalition forces from January 2006 until March 2010. The type, time and place of every registered incident is recorded; an example entry is shown in Table \ref{tab:sigactseg}. 

\begin{table}[]
\centering
\caption{Example log from the SIGACTs data}
\label{tab:sigactseg}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\bf{Date} & \bf{Category} & \bf{District} & \bf{Location} &  \bf{Province} & \bf{Region} & \bf{Type} \\ \hline
01/01/2006 & IED Found/Cleared & Zhari & 31.6x65.4 & Kandahar & RC SOUTH & Explosive Hazard \\ \hline
\end{tabular}
\end{table}

Specifically this report focuses on two specific 'categories' of event: the direct and indirect fire incidents. Direct fire is the label given to any attack in which a line of sight is available to the shooter, while indirect fire includes ballistic missiles and other weapons for which a line of sight is not required.

These were chosen due to the their abundance, providing a large and meaningful dataset, as well as their interesting relationship. The frequency of these events, binned by day, are shown in Figure \ref{fig:sigactsdata}. It should be noted that the y-axis has been restricted, concealing the number of direct and indirect fire events on day 1328, which are 204 and 182 respectively. This corresponds to 20th August 2009, the date of the Afghanistan elections; for the rest of the report this date shall be considered anomalous and removed from the data set.

\begin{figure}
\centering
\includegraphics{plot_data.png}
%[width=17cm]
\caption{Direct and Indirect Fire incidents by day}
\label{fig:sigactsdata}
\end{figure}

\section{Modelling}

This report aims to use statistical techniques to model and analyse the two data streams throughout the period. The problem can be considered one of regression, the process of determining the relationship \(f\) between inputs \(\mathbf{x}\) and noisy outputs \(\mathbf{y}\). In this case \(\mathbf{y}\) will be a training subset of the data set, or a set of observations of the data at discrete times. The remaining data can be used as a testing set, to evaluate the success of a model. \par

If the system dynamics are well understood parametric modelling is the natural course, where the form of model (for example, quartic) is chosen and the data is used to optimise the parameters. However these models have a limited expressivity; if the form of model chosen is not appropriate a good fit is impossible. As there is no obvious choice of model in this case, parametric modelling will not be explored in this report. \par

Contrastingly, non-parametric models have an expressivity which grows with the number of observations. It is always possible for the resulting function to pass through all of the training data, although in this may not be desirable. Although the form of the model is not fixed, users can incorporate traits that they believe the function to have, for example smoothness. \par

Chapter 3 of this report aims to use of Gaussian Processes to model the data, while Chapter 4 uses Log Cox Gaussian Processes. Chapter 5 investigates the presence of changepoints in the data, and Chapter 6 presents methods of evaluating and comparing the success of models. 

\chapter{Literature Review}
%A lot of work has already been done....

\section{Gaussian / Log Gaussian / Cox Processes}
This report begins by focusing on Gaussian Processes, which are defined extensively in \textit{Rasmussen and Williams, 2006} \cite{GP4ML}. Their use in modelling time series data is introduced by \textit{Roberts et. al, 2012} in \cite{GP-robots}, and the idea of modelling multiple data streams with one process is covered in \cite{multi-outputGP}.

The concept of Log Cox Gaussian Processes are introduced by \textit{M\o ller et. al, 1998} \cite{LGCP-moller} and their use in spatial and spatio-temporal data is covered in \cite{LGCP-diggle}. Using Log Cox Gaussian Processes to make predictions in the presence of changepoints is discussed by \textit{Garnett et. al} in \cite{changepoint-prediction}.

\section{Changepoints}
hi

\chapter{Gaussian Processes}

While a probability distribution describes the properties of variables, a stochastic process governs the properties of functions. \\

A Gaussian Process (hereafter referred to as a GP) is a distribution on the functions \(\mathbf{f}\) where any finite subset of function values are distributed multivariate gaussian. 
%It can be thought of as a generalisation of the multivariate gaussian distribution to a potentially infinite number of variables.

There are an infinite number of possible functions which pass through all of the training data points; the form of the resulting prediction is controlled by the choice of mean and covariance functions \(\mu (x)\) and \( k(x,x')\), which completely define the GP.

\section{Mean and Covariance Function}

Sensible choices for mean and covariance function are paramount in a model's success. The chosen functions may contain unknown parameters, to be marginalised later, however if the form of the equations is not suitable a good fit is impossible. \\ \par

The mean function represents the form a process is expected to take in the absence of any data, meaning it dictates the predictions in areas far away from any observations.

In the case of complete ignorance a zero mean function is often assumed; expressing equal likelihoods of positive and negative relationships. 

This is not suitable for this data set as a negative number of incidents is not possible. Instead a constant mean function will be assumed, such as \ref{eq:GPmean}, where the \(\theta_1\) is a hyper-parameter left to be determined.

\begin{equation} \label{eq:GPmean}
\mu (x) = \theta_1
\end{equation}

The covariance function dictates the strength of relationship between any two input points to the GP. The covariance function \(k\) is used to assemble the covariance matrix \(\mathbf{K}\) so that: 

\begin{equation}
\mathbf{K(a,b)} =  \left( \begin{array}{cccc}
k(a_1,b_1) & k(a_1,b_2) &  \dots & k(a_1,b_m) \\
k(a_2,b_1) & k(a_2,b_2) &  \dots & k(a_2,b_m) \\
: & : & : & : \\
k(a_n,b_1) & k(a_n,b_2) &  \dots & k(a_n,b_m)  \end{array} \right) 
\end{equation}

In order for \(\mathbf{K}\) to be a valid covariance matrix it needs to be positive semi-definite, meaning it satisfies \( \mathbf{v^{T} K v} \geq 0\) for any \( \mathbf{v} \neq 0 \).

In practice, this requirement is satisfied by selecting from a library of pre-existing valid covariance functions. Perhaps the most common of these, and the one which this report will focus on, is the squared exponential function.

\begin{equation}
k(x_1,x_2) = h^2 \exp^{- \frac{(x_1-x_2)^2}{2 l^2}}
\end{equation} 

This is a function of the euclidean distance between the input points and two hyper-parameters: \(h\), the output scale, which controls the amplitude of the covariance and \(l\), the length scale, which dictates the range of input values deemed to be correlated. 

\section{Regression}

One of the main attractions of the Gaussian process framework is its computational tractability. Given a set of observations \( \mathbf{y}( \mathbf{x} ) = [y_1, y_2, ... y_n] \) which correspond to the known input values \( \mathbf{x} = [x_1, x_2, .. x_n] \) we can predict the distribution for novel input vector \( \mathbf{x_*} \) as \( y( \mathbf{x_*}) \sim \mathcal{N}(\mathbf{m_*,C_*}) \) \cite{GP-robots}. Equations \ref{eq:GPmean} and \ref{eq:GPvar} can be used to find this mean and covariance, where \(\mu\) is the chosen mean function and \(\mathbf{K}\) is the assembled covariance matrix.

\singlespacing

\begin{equation} \label{eq:GPmean}
\mathbf{m_* = \mu (x_*) + K(x_* ,x) K(x,x)^{-1} (y(x) - \mu (x))}
\end{equation}

\begin{equation} \label{eq:GPvar}
\mathbf{C_* = K(x_*,x_*)-K(x_*,x) K(x,x)^{-1} K(x_*,x)}^{T}
\end{equation}

\doublespacing

Therefore a prediction can be produced for any \(\mathbf{y(x_*)}\) given only a set of observations \(\mathbf{y(x)}\). The predicted mean will go through all of the training data points, and zero variance will be predicted at these points. This is only appropriate if there is zero uncertainty in the observed data, which is often not the case.

\subsection{Observational Noise}

The SIGACTs data can be considered to be noise corrupted; inevitably some incidents will not have been recorded, and there may have been some human error in the data entry. 

Consequently it is not appropriate to assume that the training data points are known with complete confidence. This uncertainty can be incorporated by adding an unknown noise to predictions, so that the equations become \ref{eq:GP+noisemean} and \ref{eq:GP+noisevar} where \( \sigma^2 \) is the variance of the observed noise.

\singlespacing


\begin{equation} \label{eq:GP+noisemean}
\mathbf{m_*} = \boldsymbol{\mu} \mathbf{(x_*) + K(x_* ,x) (K(x,x)}+\sigma^2 \mathbf{I)^{-1} (y(x)} - \boldsymbol{\mu} \mathbf{(x))}
\end{equation}

\begin{equation} \label{eq:GP+noisevar}
\mathbf{ C_* = K(x_*,x_*)-K(x_*,x) (K(x,x)}+\sigma^2 \mathbf{I)^{-1} K(x_*,x)}^{T}
\end{equation}

\doublespacing

The resulting predicted mean will no longer necessarily go through all of the training data, and there will be some predictive uncertainty at these points.

\section{Learning Hyper-Parameters}
Using the suggested mean and covariance functions leaves 4 hyper-parameters which need to be chosen in order to produce a prediction. These are: \(\theta_1\) the mean value, \(l\) the length scale, \(h^2\) the output scale and \(\sigma^2\) the noise variance. 

The likelihood of a set of observations given a model is just equal to the multivariate Gaussian likelihood, given in \ref{eq:GPlikelihood}, where the \(p\)-length vector \(\mathbf{x}\) is the training data and the hyper-parameters are stored in a vector \(\mathbf{\theta}\).

\begin{equation} \label{eq:GPlikelihood}
L(\mathbf{x | \theta}) = \frac{1}{(2\pi)^{p/2} |\boldsymbol{\Sigma|}^{\frac{1}{2}}} exp(- \frac{1}{2} \mathbf{(x}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}))
\end{equation}

The maximum likelihood estimate for the hyper-parameters \(\mathbf{\theta}\) is the vector which maximises \ref{eq:GPlikelihood}. Given the presence of the exponent it is easier computationally to consider maximising the log-likelihood, given in equation \ref{eq:GPloglikelihood}. This is valid because \(log\) is a strictly monotonically increasing function, meaning the locations of turning points are invariant under the transformation. \par
As the first term of the log-likelihood is independent of all the hyper-parameters, the problem of finding the MLE for \(\boldsymbol{\theta}\) can be described by equation \ref{eq:GPfmin}.

\singlespacing

\begin{equation} \label{eq:GPloglikelihood}
ln(L(\mathbf{x} | \boldsymbol{\theta})) = - \frac{p}{2} ln(2\pi) - \frac{1}{2} ln(|\boldsymbol{\Sigma}|) - \frac{1}{2} (\mathbf{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})
\end{equation}

\begin{equation} \label{eq:GPfmin}
\hat{\boldsymbol{\theta}} = \argmin_\theta{\{ln(|\boldsymbol{\Sigma}|) +(\mathbf{x}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}\}
\end{equation}

\doublespacing

As dependencies between the direct and indirect fire incidents have not been considered, there will be a set of hyper-parameters for each of the data streams. \par

Optimization algorithms can compute the local minima given a function and a starting point, but it is difficult to determine when a global minima has been found. Given that the problem is in \(\mathds{R}^4\), a reasonable result should be obtained by using a grid search. This is where the optimiser is run repeatedly on using a \textit{grid} of starting points which span a range of each hyper-parameter. The lowest scoring result of these optimisations is assumed to be the global minimum. \par

\section{Results}

Once the hyper-parameters have been chosen, the mean vector and covariance matrix of the GP for a denser set of times can then be generated. This process, known as inference, allows predictions to be generated for times which do not appear in the training data set. The resulting vectors should form continuos curves, representing the underlying function which the observations are believed to be sampled from. \par

The model resulting from a training data set of 100 sample points is shown in figure \ref{fig:GPresults}. The continuous line represents the function mean and the shaded area either side shows \(\pm\) 2 standard deviations, this means ideally 95\% of the data should be inside this area. The top images plot the training data points, while the bottom ones superimpose the entire data set (the testing data) over the predictions. The hyper-parameters selected for this model are listed in table \ref{GPhyperparameters}.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=8.5cm]{GPresultsd.png}
  	\caption{Direct Fire}
  	\label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  	\centering
  	\includegraphics[width=8.5cm]{GPresultsi.png}
  	\caption{Indirect Fire}
 	 \label{fig:sub2}
\end{subfigure}
\caption{Gaussian process models from 100 sampled data points}
\label{fig:GPresults}
\end{figure}

\singlespacing
\begin{table}[]
\centering
\caption{Maximum likelihood estimates for the GP hyper-parameters}
\label{GPhyperparameters}
\begin{tabular}{c|c|c|c|c|}
\cline{2-5}
\textbf{}                                    & \(\mu\) & \(h\) & \(l\) & \(\sigma^2\) \\ \hline
\multicolumn{1}{|c|}{\textbf{Direct Fire}}   & 14.01           & 9.56          & 102.12          & 28.01              \\ \hline
\multicolumn{1}{|c|}{\textbf{Indirect Fire}} & 6.64           & 3.47          & 87.42          & 10.66              \\ \hline
\end{tabular}
\end{table}
\doublespacing

While the majority of the data set is within the 2 standard deviation points, this is representative of the large levels of uncertainty predicted at all points in the model. More alarmingly, a significant amount of probability mass is placed on negative values -- while the actual number of incidents in a day is constrained to be both positive and integer. \par 

% Also something about the direct failing to capture some characteristics near the end of the period

 

%Some bridge to the next chapter

\chapter{Log Cox Gaussian Processes}

\section{Poisson Point Processes}
The Poisson distribution is defined only over (discrete) natural numbers, and expresses the probability of a given number of events occurring in a fixed interval of time. It is defined by \ref{eq:poisson}, where \(\lambda\) is the expected number of events in a time interval, and \(x\) is the actual number observed. \cite{Barber}

\begin{equation}\label{eq:poisson}
 p(x=k|\lambda) = \mathcal{P}_o (x;\lambda) = \frac{1}{k!} e^{-\lambda} \lambda^k
\end{equation}

A poisson point process is a natural extension of this, used for modelling instances of point events, where the number of events on a given time interval is modelled as a draw from some underlying Poisson distribution, defined by a single parameter \(\lambda\). \cite{Gregory} The likelihood of a set of independent identically (Poisson) distributed observations \(\mathbf{x}=[x_1,...,x_n]^T\) is given by:

\begin{equation}
p(x_1,...,x_n|\lambda) = \prod_{i=1}^{n} p(x_i|\lambda)
\end{equation}

This would be a more natural model for the SIGACTs data, as it would only put weight on positive integer numbers. 

%These are defined by a single parameter \(\lambda\), if this is allowed to vary with time the process is labelled non-homogenous.

\subsection{Cox Processes}
A Cox process is essentially a non-homogenous Poisson point process where the underlying intensity \(\lambda\) is itself a stochastic process. In a Log Cox Gaussian process (LGCP) we assume that the log-intensity of the point process is a GP. 

\begin{equation} \label{eq:LGCPsetup}
\mathbf{v} = log(\boldsymbol{\lambda}) = \mathcal{G}\mathcal{P} ( \mu(. ;\boldsymbol{\theta}) , \mathbf{K}(. , . ,\boldsymbol{\theta}))
\end{equation}

This retains the expressivity of GPs while constraining the predicted intensities to be positive. However, the intensity values act as variables which need to be marginalised out, meaning the optimisation problem becomes significantly harder. To reduce the complexity, one of the hyper-parameters can be eliminated by selecting the mean function in \ref{eq:LGCPmean}, with the caveat that we take \(log(0)=0\). This selects the average observed log-intensity as the constant mean. 

\begin{equation} \label{eq:LGCPmean}
\mu (x) = \frac{1}{N} \sum_{i=1}^{N} log_e(y_i)
\end{equation}

The underlying characteristics of \(\mathbf{v}\) should be similar to the GP regression case, so the same form of covariance function can be used - although the optimum hyper-parameters are likely to be significantly different.

\section{Regression}

The regression problem is more complicated; as well as the mean and covariance hyper-parameters, the discrete intensity values \(\mathbf{v}\) also need to be found. The training data \(\mathbf{D} = \{y_1,y_2, ..., y_N\}\) is composed of the number of observed incidents for a sample set of days. Unlike GPs, LGCPs are not capable of producing predictions for any input vector, but only for inputs corresponding to the observations. The resulting values of \(\mathbf{v}\) will then form the predicted mean of the model. \par

Bayes rule can be used to formulate the posterior for \(\mathbf{v}\), given in \ref{eq:LCGPposterior}, which optimal values of \(v_i\) will maximise. This expression requires a prior distribution of the hyper-parameters \(p(\boldsymbol{\theta})\) which can be approximated as a delta function at the \textit{true} parameters \(\hat{\boldsymbol{\theta}}\) reducing the expression to \ref{eq:LCGPposterior2}.

\singlespacing

\begin{equation} \label{eq:LCGPposterior}
p(\mathbf{v | D}) = \frac{\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta})p(\boldsymbol{\theta}) d\boldsymbol{\theta}}}{\int{\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta})p(\boldsymbol{\theta}) d\boldsymbol{\theta} d\mathbf{v}}}}
\end{equation}

\begin{equation}
p(\boldsymbol{\theta}) = \delta(\boldsymbol{\theta} - \hat{\boldsymbol{\theta}})
\end{equation}

\begin{equation} \label{eq:LCGPposterior2}
p(\mathbf{v | D}) = \frac{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}) }{\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}) d\mathbf{v}}}
\end{equation}

\doublespacing

For a point process model, the probability distribution for each observation \(y_i\) is Poisson with intensity \(e^{v_i}\). Assuming that observations are independent the likelihood \(p(\mathbf{D|v})\) is just the product of these poisson distributions, as expressed in \ref{eq:LGCPlikelihood}. As \(\mathbf{v}\) is a GP, the distribution given a set of hyper-parameters is just a multivariate Guassian likelihood. 

\singlespacing

\begin{equation} \label{eq:LGCPlikelihood}
p(\mathbf{D|v}) = \prod_{i=1}^{m} \mathcal{P}_o (y_i, e^{v_i})
\end{equation}

\begin{equation}
p(\mathbf{v}|\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}) = \frac{1}{\sqrt{(2\pi)^{m} |\hat{\boldsymbol{\Sigma|}}}} exp(- \frac{1}{2} \mathbf{(v}-\hat{\boldsymbol{\mu}})^{T}\hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{v}-\hat{\boldsymbol{\mu}}))
\end{equation}

\doublespacing

The denominator of \ref{eq:LCGPposterior} is intractable, so an approximation is needed in order to compute the posterior. This report focuses on Laplace's approximation, where the integrand \(P(\mathbf{x})\) is assumed to be Gaussian and maximised at \(\mathbf{x=x_0}\). Equations \ref{eq:laplaceaprrox} and \ref{eq:laplaceaprrox2} give the form of the approximation. \cite{Mackay} When applied to the expression \(\int{\{p(\mathbf{D|v}) p(\mathbf{v}|\boldsymbol{\theta}=\hat{\boldsymbol{\theta}})\} d\mathbf{v}} \) the result is only evaluated at the maximum \(\hat{\mathbf{v}}\) and therefore independent of the values \(v_i\). Therefore the maxima of the posterior and the product \(p(\mathbf{D|v}) p(\mathbf{v}|\boldsymbol{\theta}=\hat{\boldsymbol{\theta}})\) are at the same location.

\singlespacing

\begin{equation} \label{eq:laplaceaprrox}
\int{P(\mathbf{x}) dx} \approx P(\mathbf{x_0}) \sqrt{\frac{2\pi}{c}}
\end{equation} 

\begin{equation} \label{eq:laplaceaprrox2}
c = - \frac{\partial^2}{\partial \mathbf{x}^2} ln P(\mathbf{x_0}) |_{\mathbf{x}=\mathbf{x_0}}
\end{equation}

\doublespacing 

We can concatenate the GP hyper-parameters and the log-intensity values into a single vector \( \boldsymbol{\gamma} = [\boldsymbol{\theta}, \mathbf{v}]^{T}\) which we can optimise over. Exploiting the monotonically increasing property of the \(log_e\) function, the problem can be written as:

\begin{equation} \label{eq:GPfmin}
\hat{\boldsymbol{\gamma}} = \argmin_\gamma{\{ \sum_{i=1}^{N}e^{v_i} - \mathbf{v}^{T}\mathbf{y} + \frac{1}{2}ln(|\boldsymbol{\Sigma}|) + \frac{1}{2}(\mathbf{v}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{v}-\boldsymbol{\mu})\}}
\end{equation}

The full posterior distribution \(p(\mathbf{v|D},\boldsymbol{\theta})\) can be approximated by the multivariate normal distribution in \ref{eq:LGCPapproxpost}. Where \(\boldsymbol{\mathcal{H}}\) is the hessian of the log distribution calculated at \(\hat{\mathbf{v}}\) 

\singlespacing
\begin{equation} \label{eq:LGCPapproxpost}
p(\mathbf{v|D},\boldsymbol{\theta}) \approx \mathcal{N} (\mathbf{v}; \hat{\mathbf{v}}, \boldsymbol{\mathcal{H}}^{-1})
\end{equation}

\begin{equation}
\boldsymbol{\mathcal{H}} = -\nabla^2 log p(\mathbf{v|D},\boldsymbol{\theta}) |_{\mathbf{v}=\hat{\mathbf{v}}}
\end{equation}
\doublespacing
% variance

\subsection{Reducing Complexity}

The optimization is now taking place in \(\mathds{R}^{N+3}\), where N is the number of training points used. As before, in order to find the global minima, the local minimum from a range of start points will be computed and the lowest valued of these is selected.

The GP model start points were distributed in 4 dimensional space meaning, given some prior knowledge of likely size of each hyper-parameter (for example that the length scale should be between 0 and the length of the full dataset) it is possible to achieve reasonable coverage of this space. However, in N+3 dimensions all points become very far apart, meaning decent coverage would require a infeasible number of start points.

While little is known about the values the noise variance, length and output scales are likely to take, the intensity values should be relatively close to the observations. 

The predicted intensities occur at times corresponding to the observed data, meaning the optimal \(v_i\) should be close to the observed \(log_e(y_i)\). The dimensionality of the start point space is reduced back to 3 if the optimiser is always started with \(\mathbf{v_0} = log_e \mathbf{y}\) so that the start point can be described by \ref{eq:LGCPstartpoint}. 


\begin{equation} \label{eq:LGCPstartpoint}
\boldsymbol{\gamma_0} = \left( \begin{array}{cc}
h_0 \\
l_0 \\
\sigma^2_0 \\
log_e(\mathbf{y}) \end{array} \right) 
\end{equation}

Only the 3 hyperparameters are varied between the different start points, however the optimization is still in \(\mathds{R}^{N+3}\) so each run of the optimiser is significantly more computationally expensive than for the GP model, meaning it may not be feasible to use the same number of start points.  \par

The number of optimization calculations can be reduced by using a latin hypercube, which given a range for each of the parameters randomly selects start points which are dissimilar. \cite{latin-hyper} This reduces the chance of multiple start points leading to the same local minimum, and more efficiently covers the start point space. \par

The tolerance used by an optimiser strongly influences the time-cost of the operation, this is the maximum change in function value between iterations at which the algorithm will stop. Many of the start points selected by the latin hypercube will be obviously sub-optimal, meaning it is wasteful to perform accurate optimization at these points. \par

This problem can be avoided by performing an initial round of optimization with a large function tolerance, which acts to identify the rough location of the global minimum. The output from this can be used as the start point for a single more precise optimization, where the local minimum found can reasonably be assumed to be global.

%cholesky decomposition

\subsection{Initial Results}

The initial predictions produced with 60 sampled training points are shown in figure \ref{fig:LGCPresults}. The smaller training set was necessitated in order to reduce computation time, but otherwise the format is the same as for the GP results. \par

All probability mass is assigned to positive numbers, and the predicted variances are much smaller - meaning the level of uncertainty has been reduced. A preliminary look at the testing plots suggests that the majority of the observed  data lies within the predicted \(\pm\) 2 standard deviation area. However, the training data shows a worrying number of points outside of this region - far more than the ideal 5\% which could suggests this model is not a good fit. 

\par
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=8.5cm]{LGCPresultsd.png}
  	\caption{Direct Fire}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  	\centering
  	\includegraphics[width=8.5cm]{LGCPresultsi.png}
  	\caption{Indirect Fire}
\end{subfigure}
\caption{Log Gaussian Cox process models from 60 sampled data points}
\label{fig:LGCPresults}
\end{figure}

The hyper-parameters learned are listed in Table \ref{LGCPhyperparameters}. Similar length scales have been chosen for the data streams, which is unsurprising as changes to direct fire rates are likely to be closely related to those in indirect fire rates. 

%%%%%

\singlespacing
\begin{table}[]
\centering
\caption{Estimates for the LGCP hyper-parameters}
\label{LGCPhyperparameters}
\begin{tabular}{c|c|c|c|}
\cline{2-4}
\textbf{}                                    & \(h\) & \(l\) & \(\sigma^2\) \\ \hline
\multicolumn{1}{|c|}{\textbf{Direct Fire}}   & 0.8068          & 63.78          & \(1.304\times 10^{-8}\)              \\ \hline
\multicolumn{1}{|c|}{\textbf{Indirect Fire}} & 0.4098          & 61.12          & \(1.5672\times 10^{-7}\)              \\ \hline
\end{tabular}
\end{table}
\doublespacing

\section{Including Correlation}

% Change this bit to include Mike's paper that i found

Given the probable correlation between quantities it seems counterintuitive to construct two independent models for the data streams; not only will the hyper-parameters likely be similar, but if the events are correlated then the training data for one stream can provide valuable information for modelling the other.\par

This section explores a single model, which takes as training data samples of both the direct and indirect fire rates and outputs predictions for both using the approach outlined in \cite{multi-outputGP}.

\subsection{Model Adaptation}

The LGCP can be easily adapted to fit a multi-input, multi-output model. The training data streams are concatenated into a single vector \ref{eq:LGCPCy}, and a step function of the form \ref{eq:LGCPCmean} can be assumed for the mean function. The first half of resulting predictions for \(\mathbf{v}\) will be for direct incidents, and the second for indirect.

\begin{equation} \label{eq:LGCPCy}
\mathbf{y} = \left( \begin{array}{cc}
\mathbf{y^{direct}} \\
\mathbf{y^{indirect}} \end{array} \right)
\end{equation}

\begin{equation} \label{eq:LGCPCmean}
\mu (x) = \begin{cases}  
\displaystyle \sum_{i=1}^{N} log_e(y_i^{direct}) & for \text{ }x\le N \\
\displaystyle \sum_{i=1}^{N}log_e(y_i^{indirect}) & for \text{ }x>N \\ \end{cases}
\end{equation}

The covariance function now takes four inputs: the time inputs \(x_1, x_2\) and the labels \(l_1,l_2\) which denote which data series the input value belongs to. For two points to be highly correlated they need to be both close in distance and the same type of incident so it is appropriate to model \(k\) as a product \ref{eq:LGCPCcov},where \(k_{SE}\) is the squared exponential function with unity variance and \(K_L\) is a square matrix whose size is equal to the number of data streams - in this case two. 

\begin{equation} \label{eq:LGCPCcov}
k(x_1,x_2,l_1,l_2) = k_{SE}(x_1,x_2) \text{ } K_L(l_1,l_2) 
\end{equation}

The squared exponential function \(k_{SE}\) is still appropriate for modelling the time component. Both the direct and indirect fire rates must have the same length scale in order for the covariance matrix to be positive semi-definite, but they may have different output scales and noise variances.

Equation \ref{eq:KL-decomp} shows how the matrix \(K_L\) can be decomposed into a vector \(\boldsymbol{\lambda}\) which contains the respective output scales of the data streams and a matrix \(\mathbf{S}\) which models the correlation between data streams. The diagonal elements of \(\mathbf{S}\) need to be 1 as a data stream is assumed to be perfectly correlated with itself, and all entries should be \(\in [0,1]\).

\singlespacing
\begin{equation} \label{eq:KL-decomp}
\begin{aligned}
K_L =& \boldsymbol{\lambda}^T \mathbf{S} \boldsymbol{\lambda} \\ \\
\boldsymbol{\lambda} = \left( \begin{array}{cc}
\rho_1  \\
\rho_2 \end{array} \right)\text{ } & \text{ }
\mathbf{S} = \left( \begin{array}{cc}
1 & \alpha  \\
\alpha  & 1 \end{array} \right)
\end{aligned}
\end{equation}
\doublespacing

The complete covariance matrix is given in \ref{eq:LGCPCcovariance} where \(\rho_1\) , \(\rho_2\) are the respective output scales, and \(\sigma_d^2\) , \(\sigma_i^2\) the noise variances.

\begin{equation} \label{eq:LGCPCcovariance}
\begin{aligned}
\boldsymbol{\Sigma} =&  \left( \begin{array}{cc}
\rho_1^2 \mathbf{K} + \sigma_i^2 \mathbf{I} & \alpha \rho_1 \rho_2 \mathbf{K}  \\
\alpha \rho_1 \rho_2 \mathbf{K} & \rho_2^2 \mathbf{K} + \sigma_d^2 \mathbf{I} \end{array} \right) \\ where\\
&\mathbf{K}(i,j) = exp\left( \frac{(x_i-x_j)^2}{l^2}\right)
\end{aligned}
\end{equation}

There are now 6 hyper-parameters to be found, which are summarised in Table \ref{LGCPChyperparameters}. For the novel correlation coefficient it may not be necessary to try multiple dissimilar start points, as empirical methods can provide a reasonable estimate of the most likely value.

Multivariate normal distributions allow simple analysis of the correlation between two randomly distributed variables. If the joint distribution of the direct and indirect fire incidents can be expressed as a 2-D gaussian, as in \ref{eq:2dmvg}, then the correlation coefficient can be approximated as \ref{eq:2dcorrelation}.

\begin{equation} \label{eq:2dmvg}
\left[ \begin{array}{cc}
y_{direct} \\
y_{indirect} \end{array} \right]
\sim
\mathcal{N} \left( \left[ \begin{array}{cc}
\mu_x \\
\mu_y \end{array} \right], \left[ \begin{array}{cc}
\sigma_{x}^2 & \sigma_{xy} \\
\sigma_{xy} & \sigma_{y}^2  \end{array} \right] \right)
\end{equation}

\begin{equation} \label{eq:2dcorrelation}
\alpha = \frac{\sigma_{xy}}{\sqrt{\sigma_{x}^2 \sigma_{y}^2}}
\end{equation}

Maximum likelihood can be used to find the best fitting 2D covariance matrix across the entire data set, the result of which is shown in \ref{fig:correlation}. This predicts a correlation of \(\alpha \approx 0.4755\).

\begin{figure}
\centering
\includegraphics[width=12cm]{direct_indirect_correlation.png}
\caption{caption me}
\label{fig:correlation}
\end{figure}

% Some positive correlation obvious, also i haven't given the figure yet

One major limitation of this method is that the covariance assigns probability mass to negative numbers. A natural extension might be to fit the 2D covariance to the log-observations and then transform the ellipse with an exponential, the results of which are illustrated in Figure\ref{fig:lcorrelation}. Although the covariance is now constrained to be positive, the ellipse has become distorted to the extent that \ref{eq:2dcorrelation} will no longer apply, and the raw correlation between the log-direct and log-indirect fire incidents isn't helpful.

\begin{figure}
\centering
\includegraphics[width=12cm]{direct_indirect_correlation2.png}
\caption{caption me}
\label{fig:lcorrelation}
\end{figure}

While the gaussian empirical correlation may not be strictly accurate, it is still reasonable to assume the true correlation is close to this value. Therefore, the latin hypercube need only be used for the other hyperparameters with the empirical value for correlation at every start point. This will reduce the number of start points necessary to ensure good coverage of the probability space, while still allowing the optimal value of correlation to be learned.

% Also conditioning errors 

The optimisation problem is now in \(\mathds{R}^{2N+6}\), meaning the size of the training data used needs to be kept relatively low to avoid excessive computation time. The size of the set is also limited by the necessary gap between observations. If training data points are too close together the corresponding rows of the covariance become very similar, and the matrix becomes poorly conditioned meaning the inverse cannot be found. \par

If the training set is too small, there is a risk the model will not capture all features of the data. To ensure this isn't happening regression can be performed on multiple independent training sets, if consistent models are predicted then the data set was large enough.

% ^ SHOULD I DO SOMETHING ACTUALLY ABOUT IT?

\begin{table}[]
\centering
\caption{Hyperparameters of the multi-input LGCP model}
\label{LGCPChyperparameters}
\begin{tabular}{ll|c}
\multicolumn{2}{c|}{\textbf{Hyperparameter}} & \textbf{Constraint} \\ \hline
\(\rho_1\)           & Direct output scale             & \(\geq0\)            \\
\(\rho_1\)             & Indirect output scale           & \(\geq0\)            \\
\(l\)          & Length scale                    & \(\geq0\)            \\
\(\alpha\)             & Correlation                     &  \(\in\{-1,1\}\)                   \\
\(\sigma_d\)             & Direct noise variance           &    \(\geq0\)                 \\
\(\sigma_i\)           & Indirect noise variance         &     \(\geq0\)               
\end{tabular}
\end{table}

\subsection{Results}
The predictions produced for a training sample of 70 points are shown in Figure \ref{fig:LGCPCresults}. The blue line represents the predicted mean of the direct data and the green the indirect, similarly the shaded areas represent the respective \(\pm\) 2 std bounds. The top plot shows only the training data points, while the bottom superimposes the predictions over the full data sets. \par

\begin{figure}
\centering
\includegraphics[width=13cm]{LGCPCresults.png}
\caption{caption me}
\label{fig:LGCPCresults}
\end{figure}

\begin{table}[]
\centering
\caption{Optimum hyperparameters for multi-input LGCP model}
\label{LGCPChyper}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\(\rho_1\)} & \multicolumn{1}{c|}{\(\rho_2\)} & \(l\)                      & \(\sigma_d^2\)            & \(\sigma_i^2\)            & \(\alpha\) \\ \hline
0.4170                            & 0.8825                           & \multicolumn{1}{c|}{41.643} & \(3.9100 \times 10^{-4}\) & \(3.3323 \times 10^{-4}\) & 0.5772     \\ \hline
\end{tabular}
\end{table}

This model appears to capture the behaviour of the data set much more successfully than the previous one, which didn't incorporate correlation between the sets. This may be largely due to the model for each data stream having effectively twice as many data training points. There are also time complexity advantages to this model, as the optimization process only has to happen once.

% Also, negative stuff?

\chapter{Changepoints}

So far this report has focused on finding a single set of hyper-parameters which can accurately describe the entire dataset. However, as the period spans four years it's likely that the system dynamics change at some point during the conflict.

Changepoints can be incorporated into regression models at the expense of added hyperparameters, and hence complexity.

\section{Sudden Change in Output Scale}

The simplest case is a step change in only one of the hyper parameters, as considered in \cite{changepoint-prediction}. As the SIGACTs data shows a sharp increase in direct fire incidents towards the end of the period, a sudden drastic change in output scale may be an appropriate model.

This can be achieved by using the altered covariance function in \ref{eq:outputcpcov} where \(k_{se}\) is the squared exponential kernel with unity output scale and \(K_L\) is the correlation matrix from the previous model and \(a\) is defined in \ref{eq:outputcpcov2}. This function increases the output scales of both data series by a factor of \(c\) after some point \(x_c\), as \(a\) is a purely injective function the resulting covariance remains positive definite. 

\singlespacing
\begin{equation} \label{eq:outputcpcov}
k(x_1,x_2,l_1,l_2) = a(x_1,x_2)\text{ } K_L(l_1,l_2) \text{ }k_{se}(x_1,x_2)
\end{equation}

\begin{equation} \label{eq:outputcpcov2}
a(x_1,x_2) = \begin{cases}
1 & for \text{ } x_1, x_2 \leq x_c \\
c & for \text{ } x_1, x_2 \geq x_c \\ 
\sqrt{c} & otherwise \\ 
\end{cases}
\end{equation}
\doublespacing

Two new hyperparameters have been introduced: the scale factor \(c\) and the changepoint location \(x_c\) which is time value at which the change occurs. As time has been discretised into days \(x_c\) must be integer, and will also represent the index of the data series at the point of change. \par
%%%% word repeat?!
The integer constraint of the changepoint location means it cannot be optimised in the same way as the other parameters, using a gradient search. One possible solution is to try every possible location at each evaluation of the model evidence however, given the number of optimization iterations necessary, this would lead to unacceptable computation times.

Another option is to iteratively optimize over the changepoint location, then the other parameters until the change in prediction becomes smaller than some predefined constant. This means for every start point from the latin hyper-cube the most likely changepoint will be found, and then used for the first round of optimization. 

For each start point we can compute the posterior \(p(x_c | D, \boldsymbol{\theta})\) where \(D\) is the training data set, \(\boldsymbol{\theta}\) is the vector of the other hyper-parameters which are the suboptimal values selected by the latin hypercube. As \(x_c\) is an index the posterior the marginal takes the form of a discrete sum, as described in \ref{eq:xcposterior}, where the joint distribution \(p(D,\boldsymbol{\theta},x_c)\) is proportional to the quantity maximised previously in equation \ref{eq:GPfmin}.

\begin{equation} \label{eq:xcposterior}
p(x_c | D, \boldsymbol{\theta}) = \frac{p(D,\boldsymbol{\theta},x_c)} {\displaystyle \sum_{i=1}^{n} p(D,\boldsymbol{\theta},x_c = t_i)}
\end{equation}

Figure \ref{fig:oscpvariation} shows this computed posterior at several stages in the optimisation process. Every 5 iterations this distribution is computed and the most likely value used for the changepoint location for the next round of optimisation. As the other parameters become more optimal the probability mass becomes more concentrated at a single location, which should be expected as this is the value around which the other variables are being optimised. This is the results from the most favourable of the start points selected by the latin hypercube.

The sudden drop-offs in the later distributions occur when the later changepoints result in poorly conditioned covariance matrices. When the optimisation is complete the covariance is poorly conditioned at almost every changepoint location besides the chosen one, meaning it is hard to quantify the certainty with which a changepoint is chosen. It should also be noted that due to the difficulty computing the later prior values, these distributions are not correctly normalised.

\begin{figure}
\centering
\includegraphics[width=16cm]{learnchangea.png}
\caption{Variation of likelihood with Changepoint location}
\label{fig:oscpvariation}
\end{figure}

% This paragraph needs re-writing
While there is a consistent peak at around 400 days this is only marginally more likely than it's surrounding values. Perhaps more alarmingly, as the optimisation progresses the peak doesn't become more defined. This suggests that this sudden changepoint model might not be a good fit for the data.

The resulting model is shown in Figure 5.2, a longer length scale and a higher correlation with respect to the previous model has been chosen. While the results still look reasonable the model evidence is lower than without the changepoint, possibly because the longer length scale is failing to capture all behaviour. 
%%% CAN"T WORK OUT HOW TO GET LABEL TO WORK PROPERLY

  \begin{figure}[!ht]
    \centering
    \includegraphics[width=11cm]{LGCP_OSCPresults.png}
    \qquad
    \doublespacing
    \begin{tabular}[b]{cc}
    \multicolumn{2}{c}{\textbf{Hyperparameters}}                                            \\ \hline
      \(\rho_1\)                    & 1.0886                \\                       
\(\rho_2\)                     & 2.5159                           \\            
\(c\)                               & 1.3279                                      \\ 
\(x_c\)                        & 374                                         \\ 
\(l\)                               & 101.14                                      \\  
\(\sigma_d^2\) & \(4.7582 \times 10^{-4}\) \\ 
\(\sigma_i^2\) & \(4.2255 \times 10^{-4}\) \\ 
\(\alpha\)                      & 0.781              \\                      
    \end{tabular}
    \captionlistentry[table]{A table beside a figure}
    \captionsetup{labelformat=andtable}
    \caption{Results for a model with direct fire output scale changepoint}
  \end{figure}


\section{Sudden Change in Correlation}

The previous model assumes a constant value of correlation between the two data streams across the changepoint. This may not be appropriate, inspection of the raw data in Figure \ref{fig:sigactsdata} appears to show increased disparity of the data streams in the later half of the time period. 

When the empirical correlation of the data streams are calculated separately for the first and second halves it is found that \(\rho_{1^{st} half} \approx 0.6077 \) and \(\rho_{2^{nd} half} \approx 0.4913\), confirming that the correlation appears to change. This difference is illustrated in Figure \ref{fig:halfcorrelation}, which plots the covariance ellipses for the two halves separately.  \par
 
\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=8.5cm]{halfcorrelation1.png}
  	\caption{First Half}
  	\label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  	\centering
  	\includegraphics[width=8.5cm]{halfcorrelation2.png}
  	\caption{Second Half}
 	 \label{fig:sub2}
\end{subfigure}
\caption{Empirical correlation of the data in two halves}
\label{fig:halfcorrelation}
\end{figure}

A sudden change in correlation can not be treated in the same way as a change in output scale; a step change in the value used would result in a covariance matrix that wasn't positive definite. One way to get around this is to directly model the difference between the data streams, instead of the individual incident types. This means the hyper-parameters learned will describe features of the relationship between the direct and indirect fire rates, rather than the data streams themselves. \par 

In order to keep both quantities being modelled positive, the two modelled quantities should be the indirect fire incidents (which are lower than the direct at every point in the data set) and the difference between the two. This is equivalent to expressing the log intensities \(v_d\) and \(v_i\) as a linear combination of two independent functions \(f\) and \(g\) which are defined in \ref{eq:ccpfandg}.

\begin{equation} 
\left[ \begin{array}{cc}
v_{d}(x)  \\
v_{i}(x) \end{array} \right] = \left[ \begin{array}{cc}
1 & 1  \\
1  & 0 \end{array} \right] \left[ \begin{array}{cc}
f(x)  \\
g(x) \end{array} \right]
\end{equation}

\begin{equation} \label{eq:ccpfandg}
\left[ \begin{array}{cc}
f(x)  \\
g(x) \end{array} \right] \sim \mathcal{G}\mathcal{P} \left( \left[ \begin{array}{cc}
\mu_i  \\
\mu_d - \mu_i \end{array} \right], \left[ \begin{array}{cc}
K_f & 0  \\
0  & K_g \end{array} \right] \right)
\end{equation}

The covariance of the indirect log-intensities \(K_f\) can take the same form used earlier in this report, where LGCPs were used to model the data streams independently, so that the covariance function is given by \ref{eq:kf} containing two hyper-parameters: the length and output scales. 

\begin{equation} \label{eq:kf}
k_f(x_1,x_2) = \rho_1 \text{ }exp\{\frac{-(x_1-x_2)^2}{2 l_1^2}\}
\end{equation}

Modelling the difference function \(g\) directly allows much more complex analysis of the change in relationship between the two data streams, which appears to change throughout the period. Therefore it is the covariance \(K_g\) which should capture the changepoint. As there is an apparent increase in the variation between the data stream the change could be modelled by an increase in the output scale. The data is still a time-series so a squared exponential covariance is still appropriate therefore \ref{eq:kg} is used, where \(a\) has the same form as in \ref{eq:outputcpcov2}.

\begin{equation} \label{eq:kg}
k_g(x_1,x_2) = \rho_2 \text{ }exp\{\frac{(x_1-x_2)^2}{2 l_2^2}\} \text{ } a(x_1,x_2)
\end{equation}


As the training data consists of observations of direct and indirect incidents, the mean vector and covariance matrix need to be transformed into this form. Using standard multivariate Gaussian identities, for a transformation of the form \(\mathbf{y}= \mathbf{M} \mathbf{x}\) these arrays are given by \(\boldsymbol{\mu_y}= \mathbf{M}\text{ }\boldsymbol{\mu_x}\) and \(\mathbf{cov_y}= \text{ }\mathbf{M}\text{ } \mathbf{cov_x} \text{ } \mathbf{M}^{T}\).par

The observations can also be assumed to be noise corrupted. In previous models maximum likelihood estimation has selected very similar values for the noise variance of the individual data streams. Therefore, in the interests of reducing the number of hyper-parameters, the noise on all observations can be modelled as the same. This means the log-intensities of the data streams can be modelled by the Gaussian process defined in \ref{eq:fandggp}. These equations are in the familiar LGCP form, so we can optimise over the hyperparameters in the same fashion as the previous model. 

\begin{equation} \label{eq:fandggp}
\left[ \begin{array}{cc}
v_d(x)  \\
v_i(x) \end{array} \right] \sim \mathcal{G}\mathcal{P} \left( \left[ \begin{array}{cc}
\mu_d  \\
\mu_i \end{array} \right], \left[ \begin{array}{cc}
K_f + K_g & K_f  \\
K_f  & K_f  \end{array} \right] + \sigma^2 \mathbf{I} \right)
\end{equation}

The evolution of the likelihood variation of changepoint is shown in Figure \ref{fig:cccpvariation}. This time a much more convincing peak at around 800 days is chosen, although conditioning errors still prevent the certainty of the final location being quantified. With each iteration less probability mass is being assigned to other changepoint values, which should be expected as the other variables are being optimised around the chosen value. This more reassuring plot suggests that a change in correlation may fit the data better than a change in output scale. \par

% Also, changepoint in much more expected place

\begin{figure}
\centering
\includegraphics[width=16cm]{learnchange.png}
\caption{Variation of likelihood with Changepoint location}
\label{fig:cccpvariation}
\end{figure}

The results of this model are shown in Figure 5.5, appearing to fit the data much better than the output scale changepoint model did. It is interesting to note that the length scale chosen for the difference function is more than twice as long as the one chosen for indirect fire.

%more about results

%% LABELLING PROBLEM WARNING NUMERO 2
  \begin{figure}[!ht]
    \centering
    \includegraphics[width=11cm]{LGCPCCPresults.png}
    \qquad
    \doublespacing
    \begin{tabular}[b]{cc}
    \multicolumn{2}{c}{\textbf{Hyperparameters}}                                            \\ \hline
      \(\rho_1\)                    & 0.3492                \\                       
\(\rho_2\)                     & 0.2285                           \\            
\(c\)                               & 2.8076                                      \\ 
\(x_c\)                        & 874                                         \\ 
\(l_1\)                               & 37.6009                                      \\  
\(l_2\) & 76.8452 \\ 
\(\sigma^2\) & \(7.1044 \times 10^{-4}\) \\ 
& \\
    \end{tabular}
    \captionlistentry[table]{A table beside a figure}
    \captionsetup{labelformat=andtable}
    \caption{Results for a model with correlation changepoint}
  \end{figure}

One notable difference between this model and its predecessors is its performance at the end of the data period. This is reassuring, given that the motivation for exploring the correlation changepoint was to model the apparent separation of the data streams in later years. The previous models all parameterised a constant correlation parameter which constrained the difference between the data stream predictions, and removing this has allowed more detail to be captured.

This model also saw a reduction in time complexity, probably due to the smaller number of hyperparameters. Unfortunately the training data set is still limited to the same size by conditioning errors, so this reduction in running time doesn't allow an increase in training data points.

\subsection{Incorporating Change in Mean}

Given that this model assumes a change in the direct fire data stream, a constant mean function no longer seems appropriate; clearly the values after the changepoint are typically higher than those before it. Currently the prior mean is set to the mean of the observed data, it should be noted that this may not be the most likely estimate for the constant mean function but this approach has been adopted to simplify the computation. \par

If the previous model was used for extrapolation, then the predicted values would quickly revert to the mean function. This means that if points from the end of the period were not included in the training data set, and these dates were predicted for the model would perform very poorly. \par

In the interests of not adding hyper-parameters a simple solution would be to model the mean function as a step change, as described in equation \ref{eq:cpmeaneq} where \(x_c\) is the time of the change point and \(n\) is its index in the \(N\)-length training data set.

\begin{equation} \label{eq:cpmeaneq}
\mu_d = \begin{cases}
\frac{1}{n} \displaystyle \sum_{i=1}^{n} log_e(y_i) & for \text{ } x \leq x_c \\
\frac{1}{N-n} \displaystyle \sum_{i=1}^{N-n} log_e(y_i) & for \text{ } x > x_c \\ 
\end{cases}
\end{equation}

Effectively the mean of the direct fire incidents before and after the changepoint are now being modelled as separate functions. This is not intuitive, but may result in a better fitting model. \par

The predictions produced by this updated model are shown in Figure 5.6 and the variation in likelihood with changepoint location is shown in Figure \ref{fig:cmcpvariation}. 
% THIRD WARNING ABOUT LABELLING, yawn....
  \begin{figure}[!ht]
    \centering
    \includegraphics[width=11cm]{LGCP_CMCP.png}
    \qquad
    \doublespacing
    \begin{tabular}[b]{cc}
    \multicolumn{2}{c}{\textbf{Hyperparameters}}                                            \\ \hline
      \(\rho_1\)                    & 1.1549                \\                       
\(\rho_2\)                     & 0.4891                           \\            
\(c\)                               & 4.8005                                      \\ 
\(x_c\)                        & 437.0000                                         \\ 
\(l_1\)                               & 33.2112                                      \\  
\(l_2\) & 87.4925 \\ 
\(\sigma^2\) & \(9.9799 \times 10^{-5}\) \\ 
& \\
    \end{tabular}
    \captionlistentry[table]{A table beside a figure}
    \captionsetup{labelformat=andtable}
    \caption{Results for a model with correlation + mean changepoint}
  \end{figure}
  
\begin{figure}
\centering
\includegraphics[width=14cm]{learnchangecm.png}
\caption{Variation of likelihood with Changepoint location}
\label{fig:cmcpvariation}
\end{figure}

The most significant difference between the models is the position of the changepoint, which the latter model predicts over a year earlier. This is especially interesting given that both models predict their respective changepoints with relative certainty and consistently. \par

This could possibly indicate the existence of more than one changepoint, or that the sudden change model is ill-fitted to the evolution of the system. 

% something else?

\chapter{Generating Predictions}

In order to properly compare the results of the models explored in this report predictions need to be generated using a common set of test data inputs \(\mathbf{x_*}\), for which observations \(\mathbf{y}_*\) are available. If each model \(i\) is used to generate \(f_i(\mathbf{x_*})\) the relative success of the algorithms in predicting \(\mathbf{y}_*\) can be assessed. \par

In this case the test inputs will be a vector of day numbers, and the models will try to predict the aggregate number of direct and indirect incidents that occurred on each of those days. The testing data should be independent of the model, so the days chosen can not have been used in the training data set of any of the models. Given the large amount of data available, this constraint is not restrictive. \par

For the GP models evaluating \(f(\mathbf{x_*})\) is trivial; full probability density functions are already being generated for every day within the period. However, for the LGCP models mean and variance values are currently only being calculated for days corresponding to the training data. \par

One possible work around is to linearly interpolate predictions from the two nearest available data points. Given the generally smooth varying nature of models this should return reasonable results, however this will not be accurate enough to distinguish between similar models. Therefore, given a LGCP model with chosen hyper-parameters predictions need to be generated for time steps not observed.
%For fair comparisons, full predictions need to be generated for the test input times using the LGCP models.

\section{LGCP Inference}

Assume a LGCP model with chosen mean and covariance functions \(\hat{\mu}\) and \(\hat{k}\) which has been generated using training data \(\mathbf{D} = (x_i,\mathbf{y_i})\) for  \(i = 1,...,m\), where \(x_i\) represents the day of the \(i\)th observation and \(\mathbf{y_i}\) contains the number of direct and indirect fire incidents on that day. Now given a new set of inputs \(x_j\) where \(j = 1,...,n\) we want to predict the number of both type of incidents on the \(j\)th day, in other word we want to generate:

\begin{equation}
p(\mathbf{v | D}) = \frac{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta}) }{\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta}) d\mathbf{v}}}\end{equation}

for a \(m+n\) length \(\mathbf{v}\) corresponding to both the observed and the test time inputs. For clarity sake we denote the new input vector \(\mathbf{x_*} = [x_i,....x_m,...,x_{m+n}]\) where the first \(m\) entries contain the times of the observed data, and the last \(n\) entries correspond to the test times. \par

The likelihood term \(p(\mathbf{D|v})\) is independent of the new test times so still has the form which was used for model optimization, repeated for convenience in \ref{eq:LCGPinflik}. The prior \(p(\mathbf{v}|\boldsymbol{\theta})\) will contain terms of \(\mathbf{v}\) relating to both the training and test data inputs, and can be expressed in terms of \(\hat{\boldsymbol{\mu}}\) and \(\hat{\boldsymbol{\Sigma}}\), the mean vector and covariance matrix generated for the input vector \(\mathbf{x_*}\) using the chosen hyper-parameters.

\singlespacing
\begin{equation} \label{eq:LCGPinflik}
p(\mathbf{D|v}) = \prod_{i=1}^{m} \mathcal{P}_o (y_i, e^{v_i})
\end{equation}

\begin{equation}
p(\mathbf{v}|\boldsymbol{\theta}) = \frac{1}{\sqrt{(2\pi)^{m+n} |\hat{\boldsymbol{\Sigma|}}}} exp(- \frac{1}{2} \mathbf{(v}-\hat{\boldsymbol{\mu}})^{T}\hat{\boldsymbol{\Sigma}}^{-1}(\mathbf{v}-\hat{\boldsymbol{\mu}}))
\end{equation}
\doublespacing

The marginal integral is intractable, so an approximation method is necessary in order to compute the posterior. Monte-Carlo methods can be used, where the integrand is approximated by sampling one distribution and inputting the results into the other. The law of large numbers implies that if this is done enough times, and the results averaged then the expectation should be representative. The approximate marginal is given by:

\begin{equation}
\int{p(\mathbf{D|v})p(\mathbf{v}|\boldsymbol{\theta}) d\mathbf{v}} \approx \frac{1}{N} \displaystyle \sum_{k=1}^{N} p(\mathbf{D|v=v_k})
\end{equation}

where \(\mathbf{v_k}\) is a draw from the distribution \(\mathcal{N}(\mathbf{v_k};\hat{\boldsymbol{\mu}},\hat{\boldsymbol{\Sigma}})\), and \(N\) is some arbitrary large number. Due to the standard form of the prior distribution, sampling can be easily achieved using inbuilt functions in Matlab. The prediction problem then becomes 

\begin{equation}
\hat{\mathbf{v}} = \argmin_\mathbf{v}{\{  \mathcal{N}(\mathbf{v};\boldsymbol{\mu, \Sigma}) \text{  }\text{  } \displaystyle \prod_{i=1}^{m} \mathcal{P}_o (y_i, e^{v_i}) \text{  }\text{  } Zp^{-1} } \}
\end{equation}

where \(Z_p\) is the approximate marginal obtained using Monte Carlo integration. Note while there are only \(m\) poisson terms in this expression, the multivariate Gaussian is in \(\mathcal{R}^{n+m}\) as it features both training and testing inputs. \par

It is simpler to maximise the log of this expression, as several terms may be neglected. Due to the fixed covariance and mean functions this optimization operation is significantly easier than those attempted earlier in this report. \par

Figure \ref{fig:inference} shows an example result of this prediction procedure. This example was for one of the later LGCP models which incorporates a change point in covariance. The predicted mean and \(\pm 2 \sigma\) are plotted in blue for the direct and green for indirect, and the training data is plotted with x's. The predictions at test times are show in red for both data streams, with o's marking the predicted mean and error bars the \(\pm 2 \sigma\) limits. For graphical clarity only four test data points have been used in this example, but similar results can be obtained for much larger data sets.

\begin{figure}[h!]
\centering
\includegraphics[width=17cm]{prediction.png}
\caption{Prediction for 4 test inputs using an LGCP model}
\label{fig:inference}
\end{figure}

Although this graph only give the predicted values with some confidence region, it is possible to generate the full probability distribution for each predicted input. Figure \ref{fig:inference2} plots in red the full posterior for the predicted direct and indirect fire incident intensities for a single test day. It should be noted that these are (continuous) distributions of the poisson intensities, not the (discrete) distributions over the number of observed incidents. 

\begin{figure}[h!]
\centering
\includegraphics[width=17cm]{prediction_dist.png}
\caption{Full posterior predictions for one test input}
\label{fig:inference2}
\end{figure}
% PAGES SORT IT OUT MOTHERFUCKER
\newpage

\chapter{Testing}

It's useful to have a formal method of evaluating and comparing the success of models. Given a set of predictions of mean and variance and a testing data set we want to assign a score reflecting the success of the model. It is important that the testing data set is independent of the training set in order to fairly analyse the model. \par

Given testing data composed of observations \(\mathbf{z_i}\) containing the number of direct and indirect fire rates on the \(i\)th day, the first task is to identify the model predictions at that time step. All models presented in this report model the posterior distribution of the number of incidents with a gaussian, so these predictions will just be a mean and variance for each data stream at every testing time step. Once the predicted distributions for each of the models have been generated results can be directly prepared.

A summary of the models produced in this report is given in Table \ref{tab:model-description}.

\begin{table}[]
\caption{Description of the models explored in this report}
\label{tab:model-description}
\centering
\begin{tabular}{c|p{14cm}}
\textbf{Model} & \multicolumn{1}{c}{\textbf{Description}}                                                                                                                                                          \\ \hline
& \\
\bf{1}              & Two independent GPs, one for each data stream                                                                                                                                 \\
& \\
\bf{2}              & Two independent LGCPs, one for each data stream                                                                                                                               \\
& \\
\bf{3}              & One multi-output LGCP which incorporates correlation between the two data streams                                                                                             \\
& \\
\bf{4}              & (3) but with a changepoint in the output scale of the direct fire incidents                                                                                                   \\
& \\
\bf{5}              & A multi-output LGCP which models the indirect fire and the difference function between the two data stream, with a changepoint in the output scale of the difference function \\
& \\
\bf{6}              & (5) but with a changepoint in the mean function at the changepoint of the difference output scale                                                                            
\end{tabular}
\end{table}

\section{Data with known Hyperparameters}

While the form of the model is paramount, it is the process of tuning the hyperparameters which fits the predictions to the data. Therefore, before making conclusions on the success of a model, we need to be sure the best fitting set of hyperparameters have been found.

One way of doing this is to generate a test data set from a model with chosen hyperparameters. For example, in the LGCP case, a vector of hyperparameters \(\boldsymbol{\theta}\) would be chosen such that the log-intensities were believed to be distributed as in \ref{eq:mvnrand}. For some vector of inputs \(\mathbf{x}\) values for \(\mathbf{v}\) would then be generated by a draw from the multivariate Gaussian distribution. These values would then be used to set up the Poisson distribution in \ref{eq:porand} which can be sampled to obtain some test observations \(\mathbf{y}\).

\singlespacing
\begin{equation} \label{eq:mvnrand}
\mathbf{v(x)} \sim \mathcal{N}\{\mu(\mathbf{x}), \mathbf{K(x},\boldsymbol{\theta})\}
\end{equation}

\begin{equation} \label{eq:porand}
\mathbf{y} \sim \mathcal{P}_o (e^{\mathbf{v}})
\end{equation}
\doublespacing

\begin{figure}
\centering
\includegraphics[width=16cm]{testmodel.png}
\caption{Four draws from the same test model}
\label{fig:testmodel}
\end{figure}

This is not a method of comparing models, but merely ensuring the full potential of a models structure is being reached. It should be noted however, that this test is flawed, as there may be two sets of hyperparameters which describe the data equally well - for example there is a trade of between length scales and observational noise. This means that while a model could be deemed to have 'failed' this test, it could still be accurately modelling the data. \par

Table \ref{tab:inmodelcompare} shows an example comparison of the true hyperparameters with those learned for model 4. In this case while all values learned are of the correct order, they are not found with exceptional accuracy. However, as discussed previously, it may be that the chosen hyperparameters describe the data equally well as the true values.

\begin{table}[]
\centering
\caption{An example of a comparison produced using data from a known model}
\label{tab:inmodelcompare}
\begin{tabular}{c|cccccccc}
                  & \(\rho_1\)      & \(\rho_2\)      & \(c\)         & \(x_c\)        & \(l\)         & \(\sigma_d^2\)    & \(\sigma_i^2\)    & \(\alpha\)     \\ \hline
\textbf{True}  &  0.6 &  0.8         &  1.2         &    800       &       100    &    \(1.0 \times 10^{-3} \)      &    \(2.0 \times 10^{-3} \)              &    0.5       \\
\textbf{Estimate} &      0.4913     &    0.8460       &    1.6596       &    724       &     81.5916      &    \(1.4436 \times 10^{-4} \)       &     \(4.0295 \times 10^{-4} \)      &      0.6124     \\ \hline
\end{tabular}
\end{table}

\section{Evaluation Methods}
The mean square error is a crude but effective evaluation tool, which measures the square distance between the training data observations and the predicted mean at that time step and sums over all testing data. The calculation is given in \ref{eq:mse}, where an N-length set of testing data has been used containing observations \(y_i\) corresponding to known inputs \(x_i\). This would produce an index of the success of the model for a single data stream, meaning scores for both the direct and indirect incident data would be obtained in each case. 

\begin{equation} \label{eq:mse}
mse = \frac{1}{N} \sum_{i=1}^{N} (y_i - f(x_i))^2
\end{equation}

While this measure gives some idea of the accuracy of the predicted mean, no account is taken of the model's predicted variance. Clearly it's preferable that the model perform badly in an area where it has declared a large uncertainty than in places where the predicted variance is low. \par

A better solution is to find the model evidence, or the probability of the testing data occurring given the model. This strictly requires the full posterior distribution for each of the generated models, which are intractable in the LGCP case. For simplicity we can assume that each observation \(y_i\) is independently Gaussian distributed, as in \ref{eq:glikelihoodobs}, with mean and variance equal to those obtained from the predictions. The log-evidence is then the sum of the log-likelihoods at all of these test observations, as given in \ref{eq:loglikelihoodobs}.

\singlespacing
\begin{equation} \label{eq:glikelihoodobs}
p(y_i) \approx \mathcal{N}(y_i;f(x_i),\sigma_i)
\end{equation}

\begin{equation} \label{eq:loglikelihoodobs}
log_e |p(D|\mathcal{M})| =  \displaystyle \sum_{i=1}^{N} log_e |p(y_i = k)|
\end{equation}
\doublespacing

The final model evidence is simply \ref{eq:loglikelihoodobs} exponentiated, however for large testing data sets this quantity may become too small to compute. One simple work around to this problem is to instead compute the average log-likelihood exponentiated, such that the score assigned to the model is given in \ref{eq:modelevidencescore}. Unlike the complete model evidence this result has no practical application, but will still award higher values to distributions which fit better. Furthermore, if the size of the testing data is held constant at \(N\) samples, this result should just be the model evidence raised to the power \(\frac{1}{N}\). 

\begin{equation} \label{eq:modelevidencescore}
score = exp \{ \frac{1}{N} \displaystyle \sum_{i=1}^{N}  log_e |p(y_i = k)|  \}
\end{equation}

Both of these scoring criteria can be used to evaluate all of the models explored in report, and each will produce a score for both the direct and indirect fire incident rates.

\section{Comparison}

Given a testing data set of 500 points, the results of both evaluation methods for each model described in Table \ref{tab:model-description} are listed in Table \ref{model-comparison}. \par

The units of the mean square error test are incidents\(^2\), so lower numbers are preferable as they signify that the testing points are closer to the predicted mean. Contrastingly the model evidence score represents the probability of a model being correct, so higher numbers are preferable.

\begin{table}[]
\centering
\caption{Formal evaluation of the different models}
\label{model-comparison}
\begin{tabular}{ccccc}
\multicolumn{1}{l}{}                                       & \multicolumn{4}{c}{{\ul \textbf{Test}}}                                                                                                                                           \\
\multicolumn{1}{c|}{\multirow{2}{*}{{ \textbf{Model}}, \(\mathcal{M}\)}} & \multicolumn{2}{c|}{\textbf{Mean Square Error}}                                         & \multicolumn{2}{c}{\textbf{Model Evidence}}                                                         \\
\multicolumn{1}{c|}{}                                      & \multicolumn{1}{c|}{\textit{Direct Fire}} & \multicolumn{1}{c|}{\textit{Indirect Fire}} & \multicolumn{1}{c|}{\textit{Direct Fire}} & \multicolumn{1}{c}{\textit{Indirect Fire}} \\ \hline
\multicolumn{1}{c|}{\textbf{1}}                    & \multicolumn{1}{c|}{29.1917}              & \multicolumn{1}{c|}{11.5186}                & \multicolumn{1}{c|}{0.0424}               & \multicolumn{1}{c}{0.0691}                 \\ \hline
\multicolumn{1}{c|}{\textbf{2}}            & \multicolumn{1}{c|}{32.9365}              & \multicolumn{1}{c|}{15.4097}                & \multicolumn{1}{c|}{0.0476}               & \multicolumn{1}{c}{0.0134}                 \\ \hline
\multicolumn{1}{c|}{\textbf{3}}    & \multicolumn{1}{c|}{25.5107}              & \multicolumn{1}{c|}{15.1999}                & \multicolumn{1}{c|}{0.0423}               & \multicolumn{1}{c}{0.1093}                 \\ \hline
\multicolumn{1}{c|}{\textbf{4}}            & \multicolumn{1}{c|}{26.6440}              & \multicolumn{1}{c|}{15.3058}                & \multicolumn{1}{c|}{0.0284}               & \multicolumn{1}{c}{0.0240}                 \\ \hline
\multicolumn{1}{c|}{\textbf{5}}    & \multicolumn{1}{c|}{25.2765}              & \multicolumn{1}{c|}{13.2748}                & \multicolumn{1}{c|}{0.0854}               & \multicolumn{1}{c}{0.0482}                 \\ \hline
\multicolumn{1}{c|}{\textbf{6}}    & \multicolumn{1}{c|}{26.2384}              & \multicolumn{1}{c|}{13.5779}                & \multicolumn{1}{c|}{0.1531}               & \multicolumn{1}{c}{0.0869}                 \\ \hline
\end{tabular}
\end{table}

Examining the numerical results immediately shows that there is no one model that scores best in all tests. \par

In general probabilistic modelling involves a compromise between model complexity and expressivity; If a model is too simple then it will not capture all features of the function it is trying to model, while an overly complex model (one with too many parameters) will spread its probability mass over too large an area, meaning it will not predict anything will significant certainty. \par

It is clear that \(\mathcal{M}=3\), the initial multi-output LGCP performs uniformly better than both \(\mathcal{M}=1,2\) which are the GP and single output LGCP models. This implies that moving to LGCPs and modelling both streams at once were both beneficial forms of added complexity. \par

Contrastingly \(\mathcal{M}=4\), where an output scale changepoint was introduced performs unanimously worse than its predecessor. This may seem counter-intuitive, as with a scale factor of 1 the models become the same, but these added redundancies increase the model uncertainty. \par

Models 5 and 6 took a different approach to modelling in order to better analyse the relationship between the two data streams, and capture a changepoint in their correlation. Unlike in \(\mathcal{M}=4\) the introduction of the changepoint has significantly improved the model evidence, suggesting that this is an appropriate model for the data, and that the added complexity has allowed the model to capture behaviour that it did not before. \par

The difference between models 5 and 6 is in the mean function; \(\mathcal{M}=5\) uses a constant mean for each data series, while \(\mathcal{M}=6\) models a step change in the direct fire mean at the position of the changepoint. This adaptation drastically improved (nearly doubled) the model evidence for both data streams, although interestingly the mean square error increased slightly. 

While it may appear obvious to select model 6 over 5, questions arise over the feasibility of the model; It is very hard to imagine an observable quantity undergoing a step change, especially one as drastic as that inferred by \(\mathcal{M}=6\). This \textit{could} be an example of a model which was designed to perform well in a test rather than meaningfully represent the data, which should be avoided. This increased score appears to indicate that the constant mean function is not optimal, if further work were to be carried out, a wider range of mean functions might be investigated. Smoother varying operators such as a sigmoid or linear function could be of use, although these come at the expense of added hyper-parameters (and therefore complexity). \par

Although it is the only thing quantified here, the quality of fit is not the only criteria for a good model. For example, time complexity and consistency of predictions are also important. \par 

By far the fastest model was \(\mathcal{M}=1\), the GP model, however this benefit is overshadowed by its drastically inferior fit. More notably, \(\mathcal{M}=5,6\) which directly modelled the difference between the data streams took significantly less time than \(\mathcal{M}=3,4\) which modelled the individual data streams. 


\chapter{Conclusion}

WILL WRITE A PROPER CONCLUSION ONCE I FIGURE OUT WHAT THE POINT WAS

--------

This report explored a subset of the SIGACTS data from the Afghanistan War. This dataset is the most detailed of its kind to be published, and modelling it may offer insight into the progression of the war. \par

The first models explored were Gaussian Processes, and although these offered complete tractability they proved ill suited to modelling deaths which, as discrete variables, are constrained to natural numbers. \par

Greater success was found using Log-Cox Gaussian Processes, which constrain predictions to be positive by placing the Gaussian Process prior on the log of the intensity function. These models were much more computationally expensive and therefore a very limited amount of training data could be used to generate the models. \par

%Analysis of the correlation between the two data st

%Changepoints were 

% Motivation: data set

% Explored models


% Which worked best


% Limitations: amount of training data 
% scalable model?

\singlespacing 

\newpage

\bibliographystyle{plain}
\bibliography{report}

\end{document}
